{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path_to_aggregated_data = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/Auxillary Data/NOAA Disaster Data/final_weather.csv\"\n",
    "\n",
    "news_weather_df = pd.read_csv(path_to_aggregated_data)\n",
    "news_weather_df.describe()\n",
    "\n",
    "print(len(news_weather_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we are going to ignore anything that got zero mentions as that will murder our data and analysis\n",
    "news_weather_df = news_weather_df[news_weather_df[\"mentions\"] > 0]\n",
    "\n",
    "# now it is time to do some modeling -- the different types of event that we can have occur are\n",
    "print(len(news_weather_df))\n",
    "print(news_weather_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to append state data to our disasters file\n",
    "\n",
    "ignore = \"Multiple states\"\n",
    "path_to_aggregated_data = path_to_aggregated_data\n",
    "path_to_gpt = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/tristan_analysis/gpt_states_affected.txt\"\n",
    "curr_event = None\n",
    "states_affected = \"\"\n",
    "\n",
    "info_dict = {}\n",
    "\n",
    "news_weather_df[\"states_affected\"] = \"\"\n",
    "\n",
    "with open(path_to_gpt, \"r\") as states_file:\n",
    "\n",
    "    lines = states_file.readlines()\n",
    "\n",
    "    lines = [item.strip() for item in lines if item.strip() != \"\"]\n",
    "\n",
    "    for l in lines:\n",
    "\n",
    "        # if there is a colon in the line, then this is the start of new_data\n",
    "        if \":\" in l:\n",
    "\n",
    "            print(curr_event)\n",
    "            print(states_affected)\n",
    "\n",
    "            if curr_event is not None:\n",
    "\n",
    "                info_dict[curr_event] = states_affected\n",
    "\n",
    "            curr_event = l[:-1]\n",
    "            states_affected = \"\"\n",
    "            continue\n",
    "            \n",
    "        elif ignore in l:\n",
    "            continue\n",
    "        \n",
    "        # otherwise this is a state that we want to analyze\n",
    "        try:\n",
    "            \n",
    "            segments = l.split(\" - \")\n",
    "            \n",
    "            # first is state\n",
    "            states_affected += segments[0].strip()\n",
    "            states_affected += \"_\"\n",
    "\n",
    "            # second is score\n",
    "            if \"(\" in segments[1]:\n",
    "                states_affected += segments[1][ : segments[1].index(\"(\")].strip()\n",
    "\n",
    "            else:\n",
    "                states_affected += segments[1].strip()\n",
    "\n",
    "            states_affected += \"|\"\n",
    "\n",
    "\n",
    "\n",
    "        except:\n",
    "\n",
    "            continue\n",
    "\n",
    "news_weather_df[\"states_affected\"] = news_weather_df.apply(lambda row: info_dict.get(row['name'], row[\"states_affected\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_aggregated_data = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/Auxillary Data/NOAA Disaster Data/final_weather_states.csv\"\n",
    "\n",
    "news_weather_df.dropna(subset=[\"states_affected\"], inplace=True)\n",
    "news_weather_df = news_weather_df[news_weather_df[\"states_affected\"] != \"\"]\n",
    "\n",
    "news_weather_df.to_csv(path_to_aggregated_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform to a state, severity mapping\n",
    "state_codes = {\n",
    "        \"Alabama\": \"AL\", \"Alaska\": \"AK\", \"Arizona\": \"AZ\", \"Arkansas\": \"AR\",\n",
    "        \"California\": \"CA\", \"Colorado\": \"CO\", \"Connecticut\": \"CT\",\n",
    "        \"Delaware\": \"DE\", \"Florida\": \"FL\", \"Georgia\": \"GA\", \"Hawaii\": \"HI\",\n",
    "        \"Idaho\": \"ID\", \"Illinois\": \"IL\", \"Indiana\": \"IN\", \"Iowa\": \"IA\",\n",
    "        \"Kansas\": \"KS\", \"Kentucky\": \"KY\", \"Louisiana\": \"LA\", \"Maine\": \"ME\",\n",
    "        \"Maryland\": \"MD\", \"Massachusetts\": \"MA\", \"Michigan\": \"MI\",\n",
    "        \"Minnesota\": \"MN\", \"Mississippi\": \"MS\", \"Missouri\": \"MO\",\n",
    "        \"Montana\": \"MT\", \"Nebraska\": \"NE\", \"Nevada\": \"NV\",\n",
    "        \"New Hampshire\": \"NH\", \"New Jersey\": \"NJ\", \"New Mexico\": \"NM\",\n",
    "        \"New York\": \"NY\", \"North Carolina\": \"NC\", \"North Dakota\": \"ND\",\n",
    "        \"Ohio\": \"OH\", \"Oklahoma\": \"OK\", \"Oregon\": \"OR\", \"Pennsylvania\": \"PA\",\n",
    "        \"Rhode Island\": \"RI\", \"South Carolina\": \"SC\", \"South Dakota\": \"SD\",\n",
    "        \"Tennessee\": \"TN\", \"Texas\": \"TX\", \"Utah\": \"UT\", \"Vermont\": \"VT\",\n",
    "        \"Virginia\": \"VA\", \"Washington\": \"WA\", \"West Virginia\": \"WV\",\n",
    "        \"Wisconsin\": \"WI\", \"Wyoming\": \"WY\"\n",
    "    }\n",
    "\n",
    "def split_state_severity(string):\n",
    "\n",
    "    pairs = string.split(\"|\")\n",
    "    pairs = [pairing.split(\"_\") for pairing in pairs if pairing != '']\n",
    "\n",
    "    ret_p = []\n",
    "\n",
    "    for p in pairs:\n",
    "        \n",
    "        try :\n",
    "\n",
    "            ret_p.append([state_codes.get(p[0], \"Unknown\"), p[1]])\n",
    "\n",
    "        except:\n",
    "\n",
    "            print(p)\n",
    "\n",
    "    return ret_p\n",
    "\n",
    "news_weather_df[\"code_severity\"] = news_weather_df[\"states_affected\"].apply(split_state_severity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_weather_df[\"code_severity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_disaster_dict = {}\n",
    "\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    print(t)\n",
    "    type_disaster_dict[t] = news_weather_df[news_weather_df[\"disaster_type\"] == t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "path_to_market_id_mapping = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/code_to_region_state_mapping.csv\"\n",
    "market_id_mapping_df = pd.read_csv(path_to_market_id_mapping)\n",
    "\n",
    "path_to_shutdown_data = \"/Users/tristanbrigham/Desktop/basic_cancelled_final copy.csv\"\n",
    "shutdown_df = pd.read_csv(path_to_shutdown_data)\n",
    "shutdown_df = shutdown_df[shutdown_df[\"amt_quarters_past\"] >= 4]\n",
    "\n",
    "markets_amt_ppl = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/markets_to_people_per_day.csv\"\n",
    "markets_amt_ppl_df = pd.read_csv(markets_amt_ppl)\n",
    "\n",
    "\n",
    "# the amount of quarters after the initial disaster that we want to check for airport closures\n",
    "max_q_count = 4\n",
    "\n",
    "# max amt passengers for small airport\n",
    "max_pass_small = 250\n",
    "max_pass_route = 100\n",
    "\n",
    "def get_airport_shutdowns_by_state_codes(name, date_start, date_end, first_quarter, code_sev_pairings):\n",
    "\n",
    "    affected_market_ids_df = pd.DataFrame(columns=market_id_mapping_df.columns)\n",
    "\n",
    "    # getting all affected market ids\n",
    "    for pair in code_sev_pairings:\n",
    "\n",
    "        affected_market_ids_df = pd.concat([affected_market_ids_df, market_id_mapping_df[market_id_mapping_df[\"state\"] == pair[0]]])\n",
    "\n",
    "\n",
    "    # now go through the data file that has the route shutdowns for 2 years after and add to a dataframe\n",
    "    c_year = date_start.year\n",
    "    c_quar = first_quarter\n",
    "    \n",
    "    \n",
    "    small_airports_total = 0\n",
    "\n",
    "    for market_id in affected_market_ids_df[\"market_id\"]:\n",
    "\n",
    "        small_airports_total += get_amt_small_airports(market_id, c_year, c_quar)\n",
    "\n",
    "\n",
    "    total_shutdowns_state_quarter = pd.DataFrame(columns = shutdown_df.columns)\n",
    "\n",
    "    q_count = 0\n",
    "\n",
    "    # go through each of the quarters and check\n",
    "    while q_count < max_q_count:\n",
    "\n",
    "        # print(f\"Looking at {c_year} : {c_quar}\")\n",
    "\n",
    "        # get all of the shutdowns that happen in those markets in that quarter\n",
    "        this_q_shutdowns_mid = shutdown_df[(shutdown_df[\"passengers\"] < max_pass_route) & (shutdown_df[\"Year\"] == c_year) & (shutdown_df[\"quarter\"] == c_quar) & ((shutdown_df['citymarketid_1'].isin(affected_market_ids_df['market_id'])) | (shutdown_df['citymarketid_2'].isin(affected_market_ids_df['market_id'])))]\n",
    "\n",
    "        # cat the market_ids\n",
    "        total_shutdowns_state_quarter = pd.concat([total_shutdowns_state_quarter, this_q_shutdowns_mid])\n",
    "\n",
    "        # increase index\n",
    "        c_quar += 1\n",
    "\n",
    "        if c_quar > 4:\n",
    "\n",
    "            c_quar = 0\n",
    "            c_year += 1\n",
    "\n",
    "        q_count += 1\n",
    "\n",
    "    return name, total_shutdowns_state_quarter, small_airports_total, len(affected_market_ids_df)\n",
    "\n",
    "        \n",
    "def get_first_quarter(q1, q2, q3, q4):\n",
    "\n",
    "    if q1 == True:\n",
    "        return 1\n",
    "\n",
    "    elif q2 == True:\n",
    "        return 2\n",
    "\n",
    "    elif q3 == True:\n",
    "        return 3\n",
    "\n",
    "    else:\n",
    "        return 4\n",
    "        \n",
    "\n",
    "def get_amt_small_airports(code, year, quarter):\n",
    "\n",
    "    return len(markets_amt_ppl_df[(markets_amt_ppl_df[\"Year\"] == year) & (markets_amt_ppl_df[\"quarter\"] == quarter) & (markets_amt_ppl_df[\"citymarketid\"] == code)  & (markets_amt_ppl_df[\"passengers\"] < max_pass_small)])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try to figure out if there is a correlation between large weather event and airports closing\n",
    "news_weather_df['start'] = pd.to_datetime(news_weather_df['start'])\n",
    "news_weather_df['end'] = pd.to_datetime(news_weather_df['end'])\n",
    "\n",
    "news_weather_df[\"first_quarter\"] = news_weather_df.apply(lambda row: get_first_quarter(row[\"Q1\"], row[\"Q2\"], row[\"Q3\"], row[\"Q4\"]), axis=1)\n",
    "\n",
    "# news_weather_df\n",
    "date_str = \"1/1/1997\"\n",
    "date_start = datetime.datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "date_str = \"1/1/2020\"\n",
    "date_end = datetime.datetime.strptime(date_str, \"%m/%d/%Y\")\n",
    "\n",
    "news_weather_df['year'] = pd.to_datetime(news_weather_df['start']).dt.year.astype(int)\n",
    "\n",
    "after_1997_weather_df = news_weather_df[(news_weather_df['start'] > date_start) & (news_weather_df['start'] < date_end)]\n",
    "\n",
    "weather_impact_exposure = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "weather_impact_normalized = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "weather_mention_exposure = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "weather_mention_normalized = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "weather_mention_ratio_exposure = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "weather_mention_ratio_normalized = {\n",
    "    \"Freeze\" : [],\n",
    "    \"Severe Storm\" : [],\n",
    "    \"Tropical Cyclone\" : [],\n",
    "    \"Wildfire\" : [],\n",
    "    \"Winter Storm\" : [],\n",
    "    \"Flooding\" : [],\n",
    "    \"Drought\" : [],\n",
    "}\n",
    "\n",
    "\n",
    "avg_dict = {\n",
    "    \"Freeze\" : [0, 0],\n",
    "    \"Severe Storm\" : [0, 0],\n",
    "    \"Tropical Cyclone\" : [0, 0],\n",
    "    \"Wildfire\" : [0, 0],\n",
    "    \"Winter Storm\" : [0, 0],\n",
    "    \"Flooding\" : [0, 0],\n",
    "    \"Drought\" : [0, 0],\n",
    "}\n",
    "\n",
    "# total_shutdowns_after_disaster_locality \n",
    "output_applied = after_1997_weather_df.apply(lambda row: get_airport_shutdowns_by_state_codes(row['name'], row['start'], row['end'], row[\"first_quarter\"], row[\"code_severity\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for disaster_name, disaster_shutdowns, small_airports, total_airports in output_applied:\n",
    "\n",
    "    # init print\n",
    "    # print(f\"Disaster Name: {disaster_name}\")\n",
    "    # print(f\"Disaster type: {news_weather_df[news_weather_df['name'] == disaster_name]['disaster_type'].values[0]}\")\n",
    "    # print(f\"Amt of shutdowns (region): {len(disaster_shutdowns)}\")\n",
    "    # print(f\"Amt of small airports (region): {small_airports}\")\n",
    "    # print(f\"Cost of disaster: {news_weather_df[news_weather_df['name'] == disaster_name]['cost'].values[0]}\")\n",
    "    # print(f\"Ratio cost to airports: {len(disaster_shutdowns) / news_weather_df[news_weather_df['name'] == disaster_name]['cost'].values[0]}\")\n",
    "    # print()\n",
    "\n",
    "    if (len(disaster_shutdowns) + 1) / (small_airports + 1) < 0.25:\n",
    "        print(f\"Disaster Name: {disaster_name}\")\n",
    "        print((len(disaster_shutdowns) + 1) / (small_airports + 1))\n",
    "\n",
    "    # print(2 * int(news_weather_df[news_weather_df[\"name\"] == disaster_name]['year'].iloc[0]))\n",
    "\n",
    "    # creating impact dict\n",
    "    weather_impact_exposure[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (small_airports + 1) , news_weather_df[news_weather_df[\"name\"] == disaster_name]['cost'].values[0], ])\n",
    "    weather_impact_normalized[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (total_airports + 1) , news_weather_df[news_weather_df[\"name\"] == disaster_name]['cost'].values[0], ])\n",
    "\n",
    "    # mentions vs. shutters\n",
    "    weather_mention_exposure[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (small_airports + 1) , news_weather_df[news_weather_df[\"name\"] == disaster_name]['mentions'].values[0] * (1.25 ** (abs(2020 - int(news_weather_df[news_weather_df[\"name\"] == disaster_name]['start'].iloc[0].year))))])\n",
    "    weather_mention_normalized[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (total_airports + 1) , news_weather_df[news_weather_df[\"name\"] == disaster_name]['mentions'].values[0] * (1.25 ** (abs(2020 - int(news_weather_df[news_weather_df[\"name\"] == disaster_name]['start'].iloc[0].year))))])\n",
    "    \n",
    "    # mention-cost relationship vs. shutters\n",
    "    weather_mention_ratio_exposure[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (small_airports + 1) ,  news_weather_df[news_weather_df[\"name\"] == disaster_name]['mentions'].values[0] * (1.25 ** (abs(2020 - int(news_weather_df[news_weather_df[\"name\"] == disaster_name]['start'].iloc[0].year)))) / news_weather_df[news_weather_df[\"name\"] == disaster_name]['cost'].values[0]])\n",
    "    weather_mention_ratio_normalized[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]].append([(len(disaster_shutdowns) + 1) / (total_airports + 1) ,  news_weather_df[news_weather_df[\"name\"] == disaster_name]['mentions'].values[0] * (1.25 ** (abs(2020 - int(news_weather_df[news_weather_df[\"name\"] == disaster_name]['start'].iloc[0].year)))) / news_weather_df[news_weather_df[\"name\"] == disaster_name]['cost'].values[0]])\n",
    "\n",
    "    # updating the averages:\n",
    "    avg_dict[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]] = [avg_dict[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]][0] + news_weather_df[news_weather_df[\"name\"] == disaster_name]['cost'].values[0], avg_dict[news_weather_df[news_weather_df[\"name\"] == disaster_name]['disaster_type'].values[0]][1] + len(disaster_shutdowns)]\n",
    "\n",
    "    # figuring out the total amount of route removals to disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_dict.items())\n",
    "\n",
    "for key, item in avg_dict.items():\n",
    "\n",
    "    print(f\"{key} : Cost: {item[0]} / Cancellations: {item[1]}\")\n",
    "    print(f\"{item[1] / item[0]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_impact_exposure[t])\n",
    "\n",
    "    x = [point[1] for point in weather_impact_exposure[t]]\n",
    "    y = [point[0] for point in weather_impact_exposure[t]]\n",
    "    \n",
    "    # # if t == \"Severe Storm\":\n",
    "    # #     x = [point[1] for point in weather_impact_exposure[t] if point[1] < 8000]\n",
    "    # #     y = [point[0] for point in weather_impact_exposure[t] if point[1] < 8000]\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "    \n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    print(f'Correlation {t} - Percent Small Airports Shuttered to Cost: {correlation_coefficient:.2f}')\n",
    "\n",
    "    \n",
    "    # # Set the title for the graph\n",
    "    # plt.title(t)\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Cost of Disaster')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_impact_normalized[t])\n",
    "\n",
    "    x = [point[1] for point in weather_impact_normalized[t]]\n",
    "    y = [point[0] for point in weather_impact_normalized[t]]\n",
    "    \n",
    "    # # if t == \"Severe Storm\":\n",
    "    # #     x = [point[1] for point in weather_impact_exposure[t] if point[1] < 8000]\n",
    "    # #     y = [point[0] for point in weather_impact_exposure[t] if point[1] < 8000]\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "    \n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    print(f'Correlation {t} - Percent Total Airports Shuttered to Cost: {correlation_coefficient:.2f}')\n",
    "\n",
    "    \n",
    "    # # Set the title for the graph\n",
    "    # plt.title(t)\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Cost of Disaster')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "\n",
    "#     max_q_count = i\n",
    "\n",
    "#     # total_shutdowns_after_disaster_locality \n",
    "#     output_applied = after_1997_weather_df.apply(lambda row: get_airport_shutdowns_by_state_codes(row['name'], row['start'], row['end'], row[\"first_quarter\"], row[\"code_severity\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_mention_normalized[t])\n",
    "\n",
    "    x = [point[1] for point in weather_mention_normalized[t]]\n",
    "    y = [point[0] for point in weather_mention_normalized[t]]\n",
    "    \n",
    "    \n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    print(f'Correlation {t} - Percent Total Airports Shuttered to Media Mentions: {correlation_coefficient:.2f}')\n",
    "\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "\n",
    "    # # Set the title for the graph\n",
    "    # plt.title(f\"Mentions vs. Shutdown: {t}\")\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Mentions')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_mention_exposure[t])\n",
    "\n",
    "    x = [point[1] for point in weather_mention_exposure[t]]\n",
    "    y = [point[0] for point in weather_mention_exposure[t]]\n",
    "    \n",
    "    \n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "    print(f'Correlation {t} - Percent Small Airports Shuttered to Media Mentions: {correlation_coefficient:.2f}')\n",
    "\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "\n",
    "    # # Set the title for the graph\n",
    "    # plt.title(f\"Mentions vs. Shutdown: {t}\")\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Mentions')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_mention_ratio_exposure[t])\n",
    "\n",
    "    x = [point[1] for point in weather_mention_ratio_exposure[t]]\n",
    "    y = [point[0] for point in weather_mention_ratio_exposure[t]]\n",
    "    \n",
    "    \n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    print(f'Correlation {t} - Percent Small Airports Shuttered to Media Mentions/Cost Ratio: {correlation_coefficient:.2f}')\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "\n",
    "    # # Set the title for the graph\n",
    "    # plt.title(f\"Mentions vs. Shutdown: {t}\")\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Mentions')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through each of the disaster categories and plot the cost of the disaster versus the amount of airports that shut down\n",
    "for t in news_weather_df[\"disaster_type\"].unique():\n",
    "\n",
    "    # print(weather_mention_ratio_normalized[t])\n",
    "\n",
    "    x = [point[1] for point in weather_mention_ratio_normalized[t]]\n",
    "    y = [point[0] for point in weather_mention_ratio_normalized[t]]\n",
    "    \n",
    "\n",
    "    correlation_coefficient = np.corrcoef(x, y)[0, 1]\n",
    "    print(f'Correlation {t} - Percent Total Airports Shuttered to Media Mentions/Cost Ratio: {correlation_coefficient:.2f}')\n",
    "    # plt.text(0.7, 0.9, f'Correlation: {correlation_coefficient:.2f}', ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "    # plt.scatter(x, y)\n",
    "\n",
    "    # # Perform linear regression to fit a line to the data\n",
    "    # coefficients = np.polyfit(x, y, 1)  # The number 1 represents linear regression (fitting a straight line)\n",
    "\n",
    "    # # Create a linear regression model based on the obtained coefficients\n",
    "    # linear_model = np.poly1d(coefficients)\n",
    "\n",
    "    # # Generate the y values of the fitted line\n",
    "    # y_fit = linear_model(x)\n",
    "\n",
    "    # # Plot the fitted line\n",
    "    # plt.plot(x, y_fit, color='red', label='Fitted Line')\n",
    "\n",
    "\n",
    "    # # Set the title for the graph\n",
    "    # plt.title(f\"Mentions vs. Shutdown: {t}\")\n",
    "\n",
    "    # # Set labels for x and y axes\n",
    "    # plt.ylabel('Airports Shut Down')\n",
    "    # plt.xlabel('Mentions')\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
