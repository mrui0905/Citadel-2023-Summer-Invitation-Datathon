{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Growth over years vs. Exit of Airlines\n",
    "\n",
    "\n",
    "Some decisions that I made here:\n",
    "\n",
    "- I am doing a syntheic control model\n",
    "- I am going to only consider data in or beyond 2003 for two reasons:\n",
    "1) 9/11 creates an artificial jump in airline route cancellations\n",
    "2) the data that I have starts in 2001 so I don't have enough data to do comparables analysis if I start too early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import k-means for analysis while doing EDA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "# the package that is going to provide us with our DID method\n",
    "# for creating a synthetic control model\n",
    "\n",
    "# we are going to be implementing a difference in differences with synthetic controls method\n",
    "from pydid import DID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETTINGS\n",
    "\n",
    "min_pass_per_day = 100\n",
    "min_ms = 0.7                # not helpful currently\n",
    "\n",
    "\n",
    "### PROBABLY DON'T TOUCH\n",
    "\n",
    "path_to_aggregated_weather_data = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/Auxillary Data/NOAA Disaster Data/final_weather.csv\"\n",
    "news_weather_df = pd.read_csv(path_to_aggregated_weather_data)\n",
    "\n",
    "path_to_economic_growth_fips = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/Auxillary Data/FIPS econ data/us_county_latlng.csv\"\n",
    "fips_mapping_df = pd.read_csv(path_to_economic_growth_fips)\n",
    "\n",
    "path_to_county_growth = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/CAGDP9/CAGDP9__ALL_AREAS_2001_2021.csv\"\n",
    "econ_growth_df = pd.read_csv(path_to_county_growth, encoding='latin-1')\n",
    "\n",
    "path_to_market_id_mapping = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/code_to_region_state_mapping.csv\"\n",
    "market_id_mapping_df = pd.read_csv(path_to_market_id_mapping)\n",
    "\n",
    "# only going to consider routes that have shut down after at least 4 quarters of operation\n",
    "path_to_shutdown_data = \"/Users/tristanbrigham/Desktop/basic_cancelled_final copy.csv\"\n",
    "shutdown_df = pd.read_csv(path_to_shutdown_data)\n",
    "shutdown_df = shutdown_df[shutdown_df[\"amt_quarters_past\"] >= 4]\n",
    "\n",
    "markets_amt_ppl = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/markets_to_people_per_day.csv\"\n",
    "markets_amt_ppl_df = pd.read_csv(markets_amt_ppl)\n",
    "\n",
    "alpha = \"abcdefghij\"\n",
    "        #0123456789\n",
    "\n",
    "\n",
    "# this function gets all of the fips codes that are affected by an air route stopping\n",
    "# it takes the market_id and looks for every fips code (county) which is in that market_id according to the mapping that we generated\n",
    "def get_affected_fips(fips_1, fips_2):\n",
    "\n",
    "    ret_arr = []\n",
    "\n",
    "    [ret_arr.append(str(x)) for x in fips_mapping_df[fips_mapping_df[\"mkt_id\"] == fips_1][\"fips_code\"].to_numpy()]\n",
    "    [ret_arr.append(str(x)) for x in fips_mapping_df[fips_mapping_df[\"mkt_id\"] == fips_2][\"fips_code\"].to_numpy()]\n",
    "\n",
    "    return ret_arr\n",
    "\n",
    "\n",
    "# this returns an alphabetical code which is analogous to the fips code\n",
    "# this gets around the issue of leading zeros in the fips code which makes it difficult for pandas to parse and find\n",
    "# the correct codes when we request them\n",
    "def custom_fips_code(fips_code):\n",
    "\n",
    "    ret_str = \"\"\n",
    "\n",
    "    fips_code = int(str(fips_code).replace('\"', ''))\n",
    "\n",
    "    if fips_code == 0:\n",
    "\n",
    "        return \"aaaaa\"\n",
    "\n",
    "    if fips_code < 10000:\n",
    "\n",
    "        ret_str += \"a\"\n",
    "\n",
    "    for digit in str(fips_code):\n",
    "\n",
    "        ret_str += alpha[int(digit)]\n",
    "    \n",
    "    return ret_str\n",
    "\n",
    "\n",
    "\n",
    "# apply the new code to the dataframe\n",
    "econ_growth_df[\"cust_id\"] = econ_growth_df[\"GeoFIPS\"].apply(custom_fips_code)\n",
    "econ_growth_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to go through and make sure that every FIPS number has an appropriate mapping\n",
    "# we are going to go through each of the shutdown routes and see what we can find\n",
    "\n",
    "# get all of the fips that were affected by this shutdown\n",
    "shutdown_df[\"fips_affected\"] = shutdown_df.apply(lambda row: get_affected_fips(row[\"citymarketid_1\"], row[\"citymarketid_2\"]), axis=1)\n",
    "\n",
    "shutdown_df = shutdown_df[shutdown_df[\"Year\"] > 2002]\n",
    "shutdown_df = shutdown_df[shutdown_df[\"passengers\"] > min_pass_per_day]\n",
    "\n",
    "print(f\"We have {len(shutdown_df)} records that we are going to consider\")\n",
    "print(f\"VAL CTS YEAR: {shutdown_df['Year'].value_counts()}\")\n",
    "# print(f\"VAL CTS YEAR: {shutdown_df['Year']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRAPHS MODELING TOTAL GDP AFTER ROUTE SHUTDOWN\n",
    "\n",
    "\n",
    "# go through and get the economic data now for each FIPS region and plot it\n",
    "# we have to get every industry and plot it\n",
    "# also have to make sure that we are getting every fips area\n",
    "for idx, row in shutdown_df.iterrows():\n",
    "\n",
    "    # get the FIPS\n",
    "    fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "    # for each of the fips idx\n",
    "    for code in fips_idx:\n",
    "\n",
    "        year_center = row[\"Year\"]\n",
    "\n",
    "        year_min = year_center - 4\n",
    "        year_max = year_center + 5\n",
    "\n",
    "        if year_min < 2001:\n",
    "            year_min = 2001\n",
    "        \n",
    "        if year_max > 2021:\n",
    "            year_max = 2021\n",
    "\n",
    "\n",
    "        get_years = list(range(year_min, year_max))\n",
    "\n",
    "\n",
    "        temp_years = []\n",
    "        for year in get_years:\n",
    "\n",
    "            if year <= 2021 and year >= 2001:\n",
    "                temp_years.append(str(year))\n",
    "\n",
    "\n",
    "        # labels = temp_years\n",
    "        # labels.append(\"GeoName\")\n",
    "        # labels.append(\"GeoFIPS\")\n",
    "        # labels.append(\"cust_id\")\n",
    "        # labels.append(\"Description\")\n",
    "\n",
    "        temp_fips = econ_growth_df[econ_growth_df[\"cust_id\"] == custom_fips_code(code)]\n",
    "        # temp_fips = temp_fips[temp_fips[\"Description\"] == \"All industry total \"]\n",
    "        temp_fips = temp_fips[temp_years]\n",
    "\n",
    "        # # create the datsets\n",
    "        # index_of_age_column = df.columns.get_loc('Age')\n",
    "\n",
    "\n",
    "        for _, sub_row in temp_fips.iterrows():\n",
    "\n",
    "            try:\n",
    "                plt.plot(get_years, [int(sub_row[lab]) for lab in temp_fips], label=sub_row)\n",
    "                # print([sub_row[lab] for lab in temp_fips])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        plt.axvline(x=year_center, color='red', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Industry Value')\n",
    "    plt.title('GDP by Year')\n",
    "    # plt.legend(title='Lines:', loc='upper left', frameon=False, prop={'size': 10})\n",
    "\n",
    "    plt.yscale('linear')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GRAPHS MODELING CHANGE IN GDP AFTER A ROUTE SHUTDOWN\n",
    "\n",
    "\n",
    "# go through and get the economic data now for each FIPS region and plot it\n",
    "# we have to get every industry and plot it\n",
    "# also have to make sure that we are getting every fips area\n",
    "# do percentage change now\n",
    "\n",
    "temp_shutdown_df = shutdown_df\n",
    "\n",
    "for idx, row in temp_shutdown_df.iterrows():\n",
    "\n",
    "    # get the FIPS\n",
    "    fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "    # for each of the fips idx\n",
    "    for code in fips_idx:\n",
    "\n",
    "        year_center = row[\"Year\"]\n",
    "\n",
    "        year_min = year_center - 4\n",
    "        year_max = year_center + 8\n",
    "\n",
    "        if year_min < 2001:\n",
    "            year_min = 2001\n",
    "        \n",
    "        if year_max > 2021:\n",
    "            year_max = 2021\n",
    "\n",
    "\n",
    "        get_years = list(range(year_min, year_max))\n",
    "\n",
    "\n",
    "        temp_years = []\n",
    "        for year in get_years:\n",
    "\n",
    "            if year <= 2021 and year >= 2001:\n",
    "                temp_years.append(str(year))\n",
    "\n",
    "\n",
    "        # labels = temp_years\n",
    "        # labels.append(\"GeoName\")\n",
    "        # labels.append(\"GeoFIPS\")\n",
    "        # labels.append(\"cust_id\")\n",
    "        # temp_years.append(\"Description\")\n",
    "\n",
    "        temp_fips = econ_growth_df[econ_growth_df[\"cust_id\"] == custom_fips_code(code)].replace('(D)', 0)\n",
    "        temp_fips = temp_fips[temp_fips[\"Description\"] == \"All industry total \"]\n",
    "        \n",
    "        desc_df = temp_fips[\"Description\"]\n",
    "        temp_fips = temp_fips[temp_years]\n",
    "\n",
    "        temp_fips = temp_fips.apply(pd.to_numeric, errors='coerce')\n",
    "        temp_fips = temp_fips.pct_change(axis=1).drop(columns=temp_years[0])\n",
    "\n",
    "        # print(desc_df)\n",
    "\n",
    "\n",
    "    #     # create the datsets\n",
    "    #     ###### index_of_age_column = df.columns.get_loc('Age')\n",
    "\n",
    "        for( _, sub_row), (label) in zip(temp_fips.iterrows(), desc_df):\n",
    "\n",
    "            # print(label)\n",
    "            # print(len(temp_years[1 : ]))\n",
    "            # print(len([sub_row[str(lab)] for lab in temp_years[1 :]]))\n",
    "\n",
    "            try:\n",
    "                # plt.plot(temp_years[1 : ], [sub_row[str(lab)] for lab in temp_years[1 :]], label=str(label))\n",
    "                y = [sub_row[str(lab)] if sub_row[str(lab)] < 2 else 2 for lab in temp_years[1 :]]\n",
    "\n",
    "                plt.plot(temp_years[1 : ], y)\n",
    "                # print([sub_row[lab] for lab in temp_fips])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        plt.axvline(x=str(year_center), color='red', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Industry Value Percent Change')\n",
    "    plt.title('GDP Percent Change by Year')\n",
    "    plt.legend(title='Lines:', loc='upper left', frameon=False, prop={'size': 4})\n",
    "\n",
    "    plt.yscale('linear')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through and get the average gdp change percentage-wise per industry per year\n",
    "econ_growth_df[\"Description\"] = econ_growth_df[\"Description\"].str.strip()\n",
    "# econ_growth_df.apply(lambda row: print(row.loc[:, 5]))\n",
    "\n",
    "temp_gdp_df = econ_growth_df.copy().fillna('(D)') \n",
    "\n",
    "custom_string = '(NA)'\n",
    "temp_gdp_df = temp_gdp_df[~temp_gdp_df.apply(lambda row: custom_string in row.values, axis=1)]\n",
    "\n",
    "\n",
    "labels = set(temp_gdp_df[\"Description\"])\n",
    "new_labels = set([l.strip() for l in labels])\n",
    "\n",
    "avg_change_year_dict = {}\n",
    "\n",
    "for label in new_labels:\n",
    "\n",
    "    curr_df = temp_gdp_df[temp_gdp_df[\"Description\"] == label]\n",
    "\n",
    "    label_avg_dict = {}\n",
    "\n",
    "    # go through and get each percentage change\n",
    "    for idx, row in curr_df.iterrows():\n",
    "\n",
    "        cols = [str(yr) for yr in list(range(2001, 2022))]\n",
    "        \n",
    "        values = row[cols]\n",
    "\n",
    "        for col_id, col_name in enumerate(cols[:-1]):\n",
    "\n",
    "            if row[cols[col_id]] != '(D)' and row[cols[col_id + 1]] != '(D)' and int(row[cols[col_id]]) != 0:\n",
    "\n",
    "                if col_name in label_avg_dict:\n",
    "                    label_avg_dict[col_name].append((int(row[cols[col_id + 1]]) / int(row[cols[col_id]])) - 1)\n",
    "                else:\n",
    "                    label_avg_dict[col_name] = [(int(row[cols[col_id + 1]]) / int(row[cols[col_id]])) - 1]\n",
    "    \n",
    "    avg_change_year_dict[label] = label_avg_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in new_labels:\n",
    "\n",
    "    for year in list(range(2001, 2021)):\n",
    "\n",
    "        try:\n",
    "            avg_change_year_dict[label][str(year)] = np.mean(avg_change_year_dict[label][str(year)])\n",
    "        except:\n",
    "            avg_change_year_dict[label][str(year)] = 0\n",
    "\n",
    "avg_change_year_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the dataframe with projected values\n",
    "cols = [str(yr) for yr in list(range(2001, 2022))]\n",
    "\n",
    "# go row by row and fill in the missing values\n",
    "for idx, row in temp_gdp_df.iterrows():\n",
    "\n",
    "    if idx % 100 == 0:\n",
    "\n",
    "        print(idx)\n",
    "\n",
    "    # check if we are missing any values\n",
    "    if '(D)' in row[cols].values:\n",
    "\n",
    "        if temp_gdp_df.at[idx, \"2001\"] != \"(D)\":\n",
    "            # first value is not a null\n",
    "\n",
    "            for col_name in cols:\n",
    "\n",
    "                if temp_gdp_df.at[idx, col_name] == '(D)':\n",
    "\n",
    "                    temp_gdp_df.at[idx, col_name] = int(temp_gdp_df.at[idx, str(int(col_name) - 1)]) * (1 + avg_change_year_dict[temp_gdp_df.at[idx, \"Description\"]][str(int(col_name) - 1)])\n",
    "\n",
    "        else:\n",
    "            # the first value is a null\n",
    "\n",
    "            if row[cols].nunique() == 1:\n",
    "\n",
    "                for col_name in cols:\n",
    "\n",
    "                    temp_gdp_df.at[idx, col_name] = 0\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # go through and find the numerical row\n",
    "                interesting_col = \"2001\"\n",
    "                for col_name in cols:\n",
    "\n",
    "                    if temp_gdp_df.at[idx, col_name] != '(D)':\n",
    "\n",
    "                        interesting_col = col_name\n",
    "                        break\n",
    "                \n",
    "                _year = int(interesting_col) + 1\n",
    "                # propogate forwards\n",
    "                while _year < 2022:\n",
    "\n",
    "                    if temp_gdp_df.at[idx, str(_year)] == '(D)':\n",
    "\n",
    "\n",
    "                        temp_gdp_df.at[idx, str(_year)] = int(int(temp_gdp_df.at[idx, str(_year - 1)]) * (1 + avg_change_year_dict[temp_gdp_df.at[idx, \"Description\"]][str(_year - 1)]))\n",
    "                    \n",
    "                    _year += 1\n",
    "\n",
    "\n",
    "                _year = int(interesting_col) - 1\n",
    "\n",
    "                # propogate backwards\n",
    "                while _year > 2000:\n",
    "\n",
    "                    temp_gdp_df.at[idx, str(_year)] = int(int(temp_gdp_df.at[idx, str(_year + 1)]) / (1 + avg_change_year_dict[temp_gdp_df.at[idx, \"Description\"]][str(_year)]))\n",
    "                    _year -= 1\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "# save the new file to csv\n",
    "temp_gdp_df.to_csv(\"~/Desktop/non_dropped_gdp_mapping.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_gdp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# getting statistics on how often each industry category drops after regional flights stop\n",
    "gdp_delta = {}\n",
    "\n",
    "labels = []\n",
    "\n",
    "diff_max = 0\n",
    "\n",
    "pre_idx_max = -1\n",
    "post_idx_max = -1\n",
    "\n",
    "### GO THROUGH TO FIND MAX TOT CHANGE\n",
    "\n",
    "for diff_pre_set in range(2, 5):\n",
    "\n",
    "    for diff_post_set in range(2, 8):\n",
    "\n",
    "        # getting statistics on how often each industry category drops after regional flights stop\n",
    "        gdp_delta = {}\n",
    "        labels = []\n",
    "        avg_arr = []\n",
    "\n",
    "        for idx, row in temp_shutdown_df.iterrows():\n",
    "\n",
    "            # get the FIPS\n",
    "            fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "            # for each of the fips idx\n",
    "            for code in fips_idx:\n",
    "\n",
    "                year_center = row[\"Year\"]\n",
    "\n",
    "                year_min = year_center - 5\n",
    "                year_max = year_center + 8\n",
    "\n",
    "                if year_min < 2001:\n",
    "                    year_min = 2001\n",
    "                \n",
    "                if year_max > 2021:\n",
    "                    year_max = 2021\n",
    "\n",
    "\n",
    "                get_years = list(range(year_min, year_max))\n",
    "\n",
    "\n",
    "                temp_years = []\n",
    "                for year in get_years:\n",
    "\n",
    "                    if year <= 2021 and year >= 2001:\n",
    "                        temp_years.append(str(year))\n",
    "\n",
    "\n",
    "                temp_fips = temp_gdp_df[temp_gdp_df[\"cust_id\"] == custom_fips_code(code)].replace('(D)', 0)\n",
    "                # temp_fips = temp_fips[temp_fips[\"Description\"] == \"All industry total \"]\n",
    "                \n",
    "                desc_df = temp_fips[\"Description\"]\n",
    "                temp_fips = temp_fips[temp_years]\n",
    "\n",
    "                temp_fips = temp_fips.apply(pd.to_numeric, errors='coerce')\n",
    "                temp_fips = temp_fips.pct_change(axis=1).drop(columns=temp_years[0]).replace(np.inf, 5).replace(-np.inf, -5).fillna(0) \n",
    "\n",
    "\n",
    "                for( _, sub_row), (label) in zip(temp_fips.iterrows(), desc_df):\n",
    "\n",
    "                    label = label.strip()\n",
    "\n",
    "                    # get the average growth for the industry in the area\n",
    "                    diff_pre = int(year_center) - int(temp_years[1])\n",
    "                    if diff_pre > diff_pre_set:\n",
    "                        diff_pre = diff_pre_set\n",
    "                    elif diff_pre <= 0:\n",
    "                        continue\n",
    "\n",
    "                    diff_post = int(temp_years[-1]) - int(year_center)\n",
    "                    if diff_post > diff_post_set:\n",
    "                        diff_post = diff_post_set\n",
    "                    elif diff_post <= 0:\n",
    "                        continue\n",
    "\n",
    "                    y_pre = sum([sub_row[str(lab)] for lab in list(range(int(year_center) - diff_pre, int(year_center)))])\n",
    "                    y_post = sum([sub_row[str(lab)] for lab in list(range(int(year_center), int(year_center) + diff_post))])\n",
    "\n",
    "                    avg_y_pre = y_pre / diff_pre\n",
    "                    avg_y_post = y_post / diff_post\n",
    "\n",
    "                    if label in gdp_delta:\n",
    "                        gdp_delta[label].append([avg_y_pre, avg_y_post, avg_y_post - avg_y_pre])\n",
    "                    else:\n",
    "                        gdp_delta[label] = [[avg_y_pre, avg_y_post, avg_y_post - avg_y_pre]]\n",
    "\n",
    "\n",
    "                    labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "        labels = set(labels)\n",
    "\n",
    "        # print(gdp_delta)\n",
    "\n",
    "        info_arr = []\n",
    "        to_csv_dict = {}\n",
    "\n",
    "        for label in labels:\n",
    "\n",
    "            temp_arr = gdp_delta[label]\n",
    "\n",
    "            pre_sum = 0\n",
    "            post_sum = 0\n",
    "            diff_sum = 0\n",
    "\n",
    "            if temp_arr == None:\n",
    "                continue \n",
    "\n",
    "            for t_set in temp_arr:\n",
    "\n",
    "                pre_sum += t_set[0]\n",
    "                post_sum += t_set[1]\n",
    "                diff_sum += t_set[2]\n",
    "            \n",
    "            pre_avg = pre_sum / len(temp_arr)\n",
    "            post_avg = post_sum / len(temp_arr)\n",
    "            diff_avg = diff_sum / len(temp_arr)\n",
    "\n",
    "            info_arr.append([label, pre_avg, post_avg, diff_avg])\n",
    "            avg_arr.append(abs(diff_avg))\n",
    "\n",
    "\n",
    "        diff_sum = sum(avg_arr)\n",
    "\n",
    "        print(f\"sum: {diff_sum} | pre_idx: {diff_pre_set} | post_idx: {diff_post_set}\")\n",
    "\n",
    "        if diff_sum > diff_max:\n",
    "\n",
    "            diff_max = diff_sum\n",
    "            pre_idx_max = diff_pre_set\n",
    "            post_idx_max = diff_post_set\n",
    "\n",
    "\n",
    "\n",
    "print(f\"MAX: pre_idx_max: {pre_idx_max} | post_idx_max: {post_idx_max}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ACTUAL CODE\n",
    "# getting statistics on how often each industry category drops after regional flights stop\n",
    "gdp_delta = {}\n",
    "labels = []\n",
    "\n",
    "for idx, row in temp_shutdown_df.iterrows():\n",
    "\n",
    "    # get the FIPS\n",
    "    fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "    # for each of the fips idx\n",
    "    for code in fips_idx:\n",
    "\n",
    "        year_center = row[\"Year\"]\n",
    "\n",
    "        year_min = year_center - 5\n",
    "        year_max = year_center + 8\n",
    "\n",
    "        if year_min < 2001:\n",
    "            year_min = 2001\n",
    "        \n",
    "        if year_max > 2021:\n",
    "            year_max = 2021\n",
    "\n",
    "\n",
    "        get_years = list(range(year_min, year_max))\n",
    "\n",
    "\n",
    "        temp_years = []\n",
    "        for year in get_years:\n",
    "\n",
    "            if year <= 2021 and year >= 2001:\n",
    "                temp_years.append(str(year))\n",
    "\n",
    "\n",
    "        # labels = temp_years\n",
    "        # labels.append(\"GeoName\")\n",
    "        # labels.append(\"GeoFIPS\")\n",
    "        # labels.append(\"cust_id\")\n",
    "        # temp_years.append(\"Description\")\n",
    "\n",
    "        temp_fips = temp_gdp_df[temp_gdp_df[\"cust_id\"] == custom_fips_code(code)].replace('(D)', 0)\n",
    "        # temp_fips = temp_fips[temp_fips[\"Description\"] == \"All industry total \"]\n",
    "        \n",
    "        desc_df = temp_fips[\"Description\"]\n",
    "        temp_fips = temp_fips[temp_years]\n",
    "\n",
    "        temp_fips = temp_fips.apply(pd.to_numeric, errors='coerce')\n",
    "        temp_fips = temp_fips.pct_change(axis=1).drop(columns=temp_years[0]).replace(np.inf, 5).replace(-np.inf, -5).fillna(0) \n",
    "\n",
    "\n",
    "        for( _, sub_row), (label) in zip(temp_fips.iterrows(), desc_df):\n",
    "\n",
    "            label = label.strip()\n",
    "\n",
    "            # get the average growth for the industry in the area\n",
    "            diff_pre = int(year_center) - int(temp_years[1])\n",
    "            if diff_pre > pre_idx_max:\n",
    "                diff_pre = pre_idx_max\n",
    "            elif diff_pre <= 0:\n",
    "                continue\n",
    "\n",
    "            diff_post = int(temp_years[-1]) - int(year_center)\n",
    "            if diff_post > post_idx_max:\n",
    "                diff_post = post_idx_max\n",
    "            elif diff_post <= 0:\n",
    "                continue\n",
    "\n",
    "            y_pre = sum([sub_row[str(lab)] for lab in list(range(int(year_center) - diff_pre, int(year_center)))])\n",
    "            y_post = sum([sub_row[str(lab)] for lab in list(range(int(year_center), int(year_center) + diff_post))])\n",
    "\n",
    "            avg_y_pre = y_pre / diff_pre\n",
    "            avg_y_post = y_post / diff_post\n",
    "\n",
    "            if label in gdp_delta:\n",
    "                gdp_delta[label].append([avg_y_pre, avg_y_post, avg_y_post - avg_y_pre])\n",
    "            else:\n",
    "                gdp_delta[label] = [[avg_y_pre, avg_y_post, avg_y_post - avg_y_pre]]\n",
    "\n",
    "\n",
    "            labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "labels = set(labels)\n",
    "\n",
    "# print(gdp_delta)\n",
    "\n",
    "info_arr = []\n",
    "to_csv_dict = {}\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    temp_arr = gdp_delta[label]\n",
    "\n",
    "    pre_sum = 0\n",
    "    post_sum = 0\n",
    "    diff_sum = 0\n",
    "\n",
    "    if temp_arr == None:\n",
    "        continue \n",
    "\n",
    "    for t_set in temp_arr:\n",
    "\n",
    "        pre_sum += t_set[0]\n",
    "        post_sum += t_set[1]\n",
    "        diff_sum += t_set[2]\n",
    "    \n",
    "    pre_avg = pre_sum / len(temp_arr)\n",
    "    post_avg = post_sum / len(temp_arr)\n",
    "    diff_avg = diff_sum / len(temp_arr)\n",
    "\n",
    "    info_arr.append([label, pre_avg, post_avg, diff_avg])\n",
    "\n",
    "\n",
    "info_arr.sort(key=lambda x: x[3])\n",
    "\n",
    "total_arr = [[], [], [], []]\n",
    "\n",
    "for label, pre_avg, post_avg, diff_avg in info_arr:\n",
    "\n",
    "    total_arr[0].append(label)\n",
    "    total_arr[1].append(pre_avg)\n",
    "    total_arr[2].append(post_avg)\n",
    "    total_arr[3].append(diff_avg)\n",
    "\n",
    "    print(f\"{diff_avg} :: \\t{label} | {pre_avg}\\t{post_avg}\")\n",
    "\n",
    "\n",
    "# to_csv_dict[\"Industry\"] = total_arr[0]\n",
    "# to_csv_dict[\"Avg Growth Before\"] = total_arr[1]\n",
    "# to_csv_dict[\"Avg Drop After\"] = total_arr[2]\n",
    "# to_csv_dict[\"Avg Difference\"] = total_arr[3]\n",
    "\n",
    "# csv_df = pd.DataFrame(to_csv_dict)\n",
    "# csv_df.to_csv(\"~/Desktop/industry_gdp_delta_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gdp_delta)\n",
    "\n",
    "info_arr = []\n",
    "to_csv_dict = {}\n",
    "\n",
    "for label in labels:\n",
    "\n",
    "    temp_arr = gdp_delta[label]\n",
    "\n",
    "    pre_sum = 0\n",
    "    post_sum = 0\n",
    "    diff_sum = 0\n",
    "\n",
    "    if temp_arr == None:\n",
    "        continue \n",
    "\n",
    "    for t_set in temp_arr:\n",
    "\n",
    "        pre_sum += t_set[0]\n",
    "        post_sum += t_set[1]\n",
    "        diff_sum += t_set[2]\n",
    "    \n",
    "    pre_avg = pre_sum / len(temp_arr)\n",
    "    post_avg = post_sum / len(temp_arr)\n",
    "    diff_avg = diff_sum / len(temp_arr)\n",
    "\n",
    "    info_arr.append([label, pre_avg, post_avg, diff_avg])\n",
    "\n",
    "info_arr.sort(key=lambda x: x[3])\n",
    "\n",
    "total_arr = [[], [], [], []]\n",
    "\n",
    "for label, pre_avg, post_avg, diff_avg in info_arr:\n",
    "\n",
    "    total_arr[0].append(label)\n",
    "    total_arr[1].append(pre_avg)\n",
    "    total_arr[2].append(post_avg)\n",
    "    total_arr[3].append(diff_avg)\n",
    "\n",
    "    print(f\"{diff_avg} :: \\t{label} | {pre_avg}\\t{post_avg}\")\n",
    "\n",
    "\n",
    "to_csv_dict[\"Industry\"] = total_arr[0]\n",
    "to_csv_dict[\"Avg Growth Before\"] = total_arr[1]\n",
    "to_csv_dict[\"Avg Drop After\"] = total_arr[2]\n",
    "to_csv_dict[\"Avg Difference\"] = total_arr[3]\n",
    "\n",
    "csv_df = pd.DataFrame(to_csv_dict)\n",
    "# csv_df.to_csv(\"~/Desktop/industry_gdp_delta_2.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pydid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for calculating disance to market id lat and lon\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from pydid import DID\n",
    "\n",
    "# first we are going to go ahead and run k-means on the file with all of the economic development \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad, lon1_rad, lat2_rad, lon2_rad = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Radius of the Earth in kilometers\n",
    "    earth_radius_km = 6371.0\n",
    "\n",
    "    # Haversine formula\n",
    "    d_lat = lat2_rad - lat1_rad\n",
    "    d_lon = lon2_rad - lon1_rad\n",
    "\n",
    "    a = sin(d_lat / 2) ** 2 + cos(lat1_rad) * cos(lat2_rad) * sin(d_lon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Calculate the distance in kilometers\n",
    "    distance_km = earth_radius_km * c\n",
    "\n",
    "    return distance_km\n",
    "\n",
    "\n",
    "# this function is going to sequentially search all of the data that we have and look for a time series\n",
    "# of growth in industries that is close to the growth that we are trying to find \n",
    "\n",
    "gdp_cols = [str(yr) for yr in list(range(2001, 2022))]\n",
    "\n",
    "\n",
    "affected_fips = []\n",
    "for idx, row in temp_shutdown_df.iterrows():\n",
    "\n",
    "    # get the FIPS\n",
    "    fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "    for fips_code in fips_idx:\n",
    "\n",
    "        affected_fips.append(custom_fips_code(fips_code))\n",
    "\n",
    "\n",
    "# creating the df that will house all of the year over year changes for gdp\n",
    "percent_change_yoy_df = temp_gdp_df.copy()\n",
    "percent_change_yoy_df[gdp_cols] = percent_change_yoy_df[gdp_cols].apply(pd.to_numeric)\n",
    "percentage_change = percent_change_yoy_df[gdp_cols].pct_change(axis=1)\n",
    "percent_change_yoy_df[gdp_cols] = percentage_change\n",
    "percent_change_yoy_df.drop(columns=\"2001\") # these will not be helpful for us\n",
    "percent_change_yoy_df.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n",
    "percent_change_yoy_df[\"route_gone\"] = percent_change_yoy_df[\"cust_id\"].apply(lambda x: x in affected_fips)\n",
    "\n",
    "affected_fips = set(affected_fips)\n",
    "\n",
    "# this is a mean-squared error approach by hand\n",
    "# we are considering all flight routes -- even the ones that have also had route attrition\n",
    "def get_closest_by_growth(fips_id, year_of_impact, first_year = 2001, same_time_frame=True, num_to_find=20):\n",
    "\n",
    "    # get all of the right characteristics   \n",
    "\n",
    "    # fill in the dataframe with projected values\n",
    "    # since the percentage change columns have the percentage change from the last year to the year that the value is in, we are going to consider those columns\n",
    "    cols = [str(yr) for yr in list(range(first_year + 1, year_of_impact))]\n",
    "\n",
    "    # get the comparison dataframe\n",
    "    base_information = percent_change_yoy_df[percent_change_yoy_df[\"cust_id\"] == custom_fips_code(fips_id)][cols].copy()\n",
    "\n",
    "    # min MSE\n",
    "    min_mse_val = [float('inf') for i in range(num_to_find)]\n",
    "    min_mse_fips = [-1 for i in range(num_to_find)]\n",
    "\n",
    "    # we will iterate through all of the possible fips that we have data on, and calculate the mean squared error for the dataframe values\n",
    "    set_unique = set(percent_change_yoy_df[\"cust_id\"].unique())\n",
    "    \n",
    "    for search_fips in list(set_unique):\n",
    "\n",
    "        if search_fips == custom_fips_code(fips_id):\n",
    "            continue\n",
    "\n",
    "        # get the percent changes for each fips\n",
    "        test_fips_df = percent_change_yoy_df[percent_change_yoy_df[\"cust_id\"] == search_fips][cols].copy()\n",
    "\n",
    "        # MSE against original\n",
    "        \n",
    "        base_information = base_information.apply(pd.to_numeric, errors='coerce')\n",
    "        base_information = base_information.reset_index(drop=True)\n",
    "        \n",
    "        test_fips_df = test_fips_df.apply(pd.to_numeric, errors='coerce')\n",
    "        test_fips_df = test_fips_df.reset_index(drop=True)\n",
    "\n",
    "        # manually calculate the mse\n",
    "        total_value = 0\n",
    "\n",
    "        squared_differences = base_information - test_fips_df\n",
    "        squared_differences = squared_differences ** 2\n",
    "\n",
    "        # Calculate the Mean Squared Error for each column\n",
    "        mse_by_column = squared_differences.mean()\n",
    "\n",
    "        # Calculate the overall Mean Squared Error\n",
    "        overall_mse = mse_by_column.mean()\n",
    "\n",
    "        # check if the new mse is less than before\n",
    "        if overall_mse < max(min_mse_val):\n",
    "\n",
    "            mse_ind = min_mse_val.index(max(min_mse_val))\n",
    "\n",
    "            min_mse_fips[mse_ind] = search_fips\n",
    "            min_mse_val[mse_ind] = overall_mse\n",
    "    \n",
    "    # now I should have an array of all of the cities that are closest in terms of percentage growth\n",
    "\n",
    "\n",
    "    return min_mse_fips, min_mse_val\n",
    "\n",
    "percent_change_yoy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to make sure that we have enough data, we will augment in the following ways:\n",
    "# 1) closest 3 cities to this city by lat-lon + their GDP's and GDP growth\n",
    "# 2) lat and lon of the other cities\n",
    "# 3) distance to the other cities\n",
    "\n",
    "# let's start by augmenting our dataframe of stopped routes cities\n",
    "mse_dictionary = {}\n",
    "\n",
    "# getting statistics on how often each industry category drops after regional flights stop\n",
    "for idx, row in temp_shutdown_df.iterrows():\n",
    "\n",
    "    # get the FIPS\n",
    "    fips_idx = row[\"fips_affected\"]\n",
    "\n",
    "    id_dict = {}\n",
    "\n",
    "    # for each of the fips idx\n",
    "    for code in fips_idx:\n",
    "\n",
    "        print(f\"ANALYZE: {custom_fips_code(code)}\")\n",
    "\n",
    "        closest_similarity_cities, closest_similarity_errors = get_closest_by_growth(code, row[\"Year\"], first_year = 2001, same_time_frame=True, num_to_find=50)\n",
    "        id_dict[custom_fips_code(code)] = sorted(list(zip(closest_similarity_cities, closest_similarity_errors)), key=lambda x: x[1])\n",
    "\n",
    "        print(closest_similarity_cities)\n",
    "        print(closest_similarity_errors)\n",
    "\n",
    "    mse_dictionary[row[\"citadel_id\"]] = id_dict \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle the information just incase anything happens down the line:\n",
    "# import pickle\n",
    "\n",
    "import copy\n",
    "\n",
    "backup_dict = copy.deepcopy(mse_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "backup_dict2 = copy.deepcopy(mse_dictionary)\n",
    "\n",
    "# print(os.getcwd())\n",
    "\n",
    "# with open(\"./pickle_files/mse_comps_for_flights_dictionary.pkl\", \"wb\") as file:\n",
    "#     pickle.dump(mse_dictionary, file)\n",
    "\n",
    "# open_f = open(f\"~/Desktop/mse_comps_for_flights_dictionary.pkl\", \"wb\")\n",
    "# pickle.dump(mse_dictionary, open_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    "    mse_dictionary = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we are going to prepare dataframes for synthetic control models below\n",
    "\n",
    "route_and_affected_fips = {}\n",
    "scm_dataframe_dict = {}\n",
    "\n",
    "# here is the control flow\n",
    "# we are going to start by going through each of the keys\n",
    "# get that data from the percent change \n",
    "# go through each of the constituent comparable models\n",
    "# get their probabilities\n",
    "# for each of the types of gdp that we consider, we collect all the information\n",
    "# pass the function below each type of gdp for each of the control and treated regions\n",
    "# that will create the plots that we need\n",
    "\n",
    "# mse dict is indexed by route code\n",
    "# dict in there is indexed by fips code\n",
    "# that has an array of similar fips and their mse\n",
    "\n",
    "def get_mse_score(fips_id, dict):\n",
    "\n",
    "    return dict[fips_id]\n",
    "\n",
    "\n",
    "# iterate through each of the keys\n",
    "for route_id in mse_dictionary.keys():\n",
    "\n",
    "    # get the list of comparables in order\n",
    "    # dictionary of affected fips id's and their closest parters w mse's\n",
    "    closest_matches = mse_dictionary[route_id]\n",
    "    year_affected = temp_shutdown_df.loc[temp_shutdown_df[\"citadel_id\"] == route_id, \"Year\"].values[-1]\n",
    "\n",
    "    route_and_affected_fips[route_id] = (closest_matches.keys(), year_affected)\n",
    "    scm_dataframe_dict[route_id] = {}\n",
    "\n",
    "    # go through each of the fips that were affected by the route closure\n",
    "    for affected_fips_code in closest_matches.keys():\n",
    "\n",
    "        # create a dataframe\n",
    "        temp_route_scm_df = percent_change_yoy_df[percent_change_yoy_df[\"cust_id\"] == affected_fips_code].copy()\n",
    "        temp_route_scm_df[\"mse_score\"] = 0\n",
    "\n",
    "        # now go through each of the ids and get their data\n",
    "        for (fips_id, mse) in closest_matches[affected_fips_code]:\n",
    "\n",
    "            temp_route_scm_df = pd.concat([temp_route_scm_df, percent_change_yoy_df[percent_change_yoy_df[\"cust_id\"] == fips_id]])\n",
    "            temp_route_scm_df.loc[temp_route_scm_df[\"cust_id\"] == fips_id, \"mse_score\"] = mse\n",
    "\n",
    "        # temp_route_scm_df[\"mse_score\"] = temp_route_scm_df.apply(lambda row: get_mse_score(row[\"citadel_id\"], closest_matches))\n",
    "        # temp_route_scm_df[\"route_deprecated\"] = (temp_route_scm_df[\"citadel_id\"] == route_id)\n",
    "        temp_route_scm_df[\"affected\"] = (temp_route_scm_df[\"cust_id\"] == affected_fips_code)\n",
    "        scm_dataframe_dict[route_id][affected_fips_code] = temp_route_scm_df.copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_dataframe_dict[3150332467]['agaad']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install statsmodels\n",
    "%pip install pmdarima\n",
    "%pip install SyntheticControlMethods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from pmdarima import auto_arima\n",
    "from SyntheticControlMethods import Synth, DiffSynth\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now go through each of the items in the dictionary and create stats models for each of them\n",
    "\n",
    "\n",
    "# this function creates the synthetic control model and plots the data\n",
    "# to visualize the effect of route attrition\n",
    "total_year_cols = [str(yr) for yr in list(range(2001, 2022))]\n",
    "\n",
    "def synthetic_control_model(pct_chng_df, route_attrition_year, response_gdp, affected_fips_id):\n",
    "\n",
    "    # we need to transpose the matrix that we are given to start\n",
    "    # that way the description gdp is one of the columns\n",
    "    # and the years are the indexes\n",
    "\n",
    "    temp_cols = [\"Description\"]\n",
    "    \n",
    "    for year_col in total_year_cols:\n",
    "        temp_cols.append(year_col)\n",
    "\n",
    "\n",
    "\n",
    "    # get rid of any data that has a changed route\n",
    "    arr_1 = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(affected_fips_id)]\n",
    "    arr_2 = pct_chng_df[pct_chng_df[\"route_gone\"] == False]\n",
    "\n",
    "    pct_chng_df = pd.concat([arr_1, arr_2]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # we are going to go index by index, transpose the matrix, and add the other columns back in as necessary\n",
    "    fips_ids = pct_chng_df[\"GeoFIPS\"].unique()\n",
    "\n",
    "    # df that we are going to be writing to\n",
    "    total_input_data = pd.DataFrame(columns=[\"Year\", response_gdp, \"GeoFIPS\", \"cust_id\"])\n",
    "\n",
    "    # go through each of the fips ids and get the data that is associated with them\n",
    "    for fips_id in fips_ids:\n",
    "\n",
    "        # get the data that is associated with that fips id\n",
    "        temp_fips_df = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(fips_id)]\n",
    "\n",
    "        # get the transpose of the matrix\n",
    "        temp_fips_df = temp_fips_df[temp_cols]\n",
    "        temp_fips_df = temp_fips_df.T\n",
    "\n",
    "        # set the new column headers\n",
    "        new_column_headers = temp_fips_df.iloc[0]\n",
    "        temp_fips_df = temp_fips_df[1:]\n",
    "\n",
    "\n",
    "        # Set the new column headers and rename the axis\n",
    "        temp_fips_df.columns = new_column_headers\n",
    "        temp_fips_df = temp_fips_df.rename_axis(None, axis=1)\n",
    "\n",
    "        # rename and get only the interesting data\n",
    "        temp_fips_df = temp_fips_df[response_gdp].reset_index().rename(columns = {\"index\" : \"Year\"})\n",
    "\n",
    "        # add the ids\n",
    "        temp_fips_df[\"GeoFIPS\"] = str(fips_id)\n",
    "        temp_fips_df[\"cust_id\"] = str(custom_fips_code(fips_id))\n",
    "\n",
    "        total_input_data = pd.concat([total_input_data, temp_fips_df])\n",
    "\n",
    "        \n",
    "    # go ahead and adjust the data frame\n",
    "    total_input_data[\"Year\"] = total_input_data[\"Year\"].apply(pd.to_numeric, errors='coerce')\n",
    "    total_input_data[\"target\"] = total_input_data[\"cust_id\"] == custom_fips_code(affected_fips_id)\n",
    "    total_input_data[\"after_treatment\"] = total_input_data[\"Year\"] >= int(route_attrition_year)\n",
    "\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "\n",
    "    (total_input_data\n",
    "        .assign(target = np.where(total_input_data[\"target\"], \"Target\", \"Other Counties\"))\n",
    "        .groupby([\"Year\", \"target\"])[response_gdp]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .pivot(index=\"Year\", columns=\"target\", values=response_gdp)\n",
    "        .plot(ax=ax, figsize=(10,5)))\n",
    "    \n",
    "\n",
    "\n",
    "    inverted = (total_input_data.query(\"~after_treatment\") # filter pre-intervention period\n",
    "            .pivot(index='cust_id', columns=\"Year\")[response_gdp] # make one column per year and one row per state\n",
    "            .T) # flip the table to have one column per state\n",
    "\n",
    "    y = inverted[custom_fips_code(affected_fips_id)].values    \n",
    "    X = inverted.drop(columns=custom_fips_code(affected_fips_id)).values  # other states \n",
    "\n",
    "    weights_lr = Lasso(fit_intercept=False).fit(X, y).coef_.round(3)\n",
    "\n",
    "    print(weights_lr)\n",
    "\n",
    "    synthetic = (total_input_data.query(\"~target\")\n",
    "                  .pivot(index='Year', columns=\"cust_id\")[response_gdp]\n",
    "                  .values.dot(weights_lr))\n",
    "    \n",
    "\n",
    "    return (total_input_data\n",
    "            .query(f\"cust_id=={custom_fips_code(affected_fips_id)}\")[[\"cust_id\", \"Year\", response_gdp, \"after_treatment\"]]\n",
    "            .assign(synthetic=synthetic))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def avg_SCM(pct_chng_df, route_attrition_year, response_gdp, affected_fips_id, pre, post, quarter):\n",
    "\n",
    "    # we need to transpose the matrix that we are given to start\n",
    "    # that way the description gdp is one of the columns\n",
    "    # and the years are the indexes\n",
    "\n",
    "    temp_cols = [\"Description\"]\n",
    "    \n",
    "    for year_col in total_year_cols:\n",
    "        temp_cols.append(year_col)\n",
    "\n",
    "\n",
    "\n",
    "    # get rid of any data that has a changed route\n",
    "    arr_1 = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(affected_fips_id)]\n",
    "    arr_2 = pct_chng_df[pct_chng_df[\"route_gone\"] == False]\n",
    "\n",
    "    pct_chng_df = pd.concat([arr_1, arr_2]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # we are going to go index by index, transpose the matrix, and add the other columns back in as necessary\n",
    "    fips_ids = pct_chng_df[\"GeoFIPS\"].unique()\n",
    "\n",
    "    # df that we are going to be writing to\n",
    "    total_input_data = pd.DataFrame(columns=[\"Year\", response_gdp, \"GeoFIPS\", \"cust_id\"])\n",
    "\n",
    "    # go through each of the fips ids and get the data that is associated with them\n",
    "    for fips_id in fips_ids:\n",
    "\n",
    "        # get the data that is associated with that fips id\n",
    "        temp_fips_df = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(fips_id)]\n",
    "\n",
    "        # get the transpose of the matrix\n",
    "        temp_fips_df = temp_fips_df[temp_cols]\n",
    "        temp_fips_df = temp_fips_df.T\n",
    "\n",
    "        # set the new column headers\n",
    "        new_column_headers = temp_fips_df.iloc[0]\n",
    "        temp_fips_df = temp_fips_df[1:]\n",
    "\n",
    "\n",
    "        # Set the new column headers and rename the axis\n",
    "        temp_fips_df.columns = new_column_headers\n",
    "        temp_fips_df = temp_fips_df.rename_axis(None, axis=1)\n",
    "\n",
    "        # rename and get only the interesting data\n",
    "        temp_fips_df = temp_fips_df[response_gdp].reset_index().rename(columns = {\"index\" : \"Year\"})\n",
    "\n",
    "        # add the ids\n",
    "        temp_fips_df[\"GeoFIPS\"] = str(fips_id)\n",
    "        temp_fips_df[\"cust_id\"] = str(custom_fips_code(fips_id))\n",
    "\n",
    "        total_input_data = pd.concat([total_input_data, temp_fips_df])\n",
    "\n",
    "        \n",
    "    # go ahead and adjust the data frame\n",
    "    total_input_data[\"Year\"] = total_input_data[\"Year\"].apply(pd.to_numeric, errors='coerce')\n",
    "    total_input_data[\"target\"] = total_input_data[\"cust_id\"] == custom_fips_code(affected_fips_id)\n",
    "    total_input_data[\"after_treatment\"] = total_input_data[\"Year\"] >= int(route_attrition_year)\n",
    "\n",
    "    # plt.figure(figsize=(10,6))\n",
    "    # plt.plot(total_input_data.query(\"target\")[\"Year\"], total_input_data.query(\"target\")[response_gdp], label=\"Target\")\n",
    "    # plt.plot(total_input_data.query(\"target\")[\"Year\"], routes_synth_lr, label=\"Synthetic Control\")\n",
    "    # plt.vlines(x=route_attrition_year, ymin=-1, ymax=1, linestyle=\":\", lw=2, label=\"Route Attrition\")\n",
    "    # plt.ylabel(\"GDP Growth Over Years\")\n",
    "    # plt.legend()\n",
    "\n",
    "    # sc = Synth(total_input_data, response_gdp, \"GeoFIPS\", \"Year\", route_attrition_year, affected_fips_id, n_optim=100)\n",
    "    # sc.plot([\"original\", \"pointwise\", \"cumulative\"], treated_label=affected_fips_id, synth_label=\"Synthetic County\", treatment_label=\"Route Closure\")\n",
    "\n",
    "\n",
    "    treated_df = total_input_data[total_input_data['cust_id'] == custom_fips_code(affected_fips_id)]\n",
    "    control_df = total_input_data[~(total_input_data['cust_id'] == custom_fips_code(affected_fips_id))]\n",
    "\n",
    "\n",
    "    # # Separate pre- and post-policy data\n",
    "    # pre_policy = total_input_data[total_input_data['Year'] < int(route_attrition_year)]\n",
    "    # post_policy = total_input_data[total_input_data['Year'] >= int(route_attrition_year)]\n",
    "\n",
    "\n",
    "    # Fit ARIMA model for each control region\n",
    "    # # control_arima_models = {}\n",
    "    # print(F\"AFFECTED REGIONS: {affected_fips_id}\")\n",
    "    # print(F\"CONTROL REGIONS: {list(set(fips_ids))}\")\n",
    "\n",
    "\n",
    "    # # go through each of the data sets\n",
    "    # for untouched_id in list(set(fips_ids)):\n",
    "\n",
    "    #     if untouched_id == affected_fips_id:\n",
    "    #         continue\n",
    "\n",
    "    #     # get untouched regions\n",
    "    #     region_df = control_df[control_df['cust_id'] == custom_fips_code(untouched_id)]\n",
    "\n",
    "    #     # gridsearch for the model\n",
    "    #     control_arima_models[untouched_id] = auto_arima(region_df[response_gdp], seasonal=False)\n",
    "\n",
    "\n",
    "    # # Fit ARIMA model for the treated region\n",
    "    # treated_arima_model = auto_arima(treated_df[response_gdp], seasonal=False)\n",
    "\n",
    "\n",
    "    # # Predict using ARIMA models for control regions\n",
    "    # control_predictions = {}\n",
    "    # for region, model in control_arima_models.items():\n",
    "\n",
    "    #     control_predictions[region] = model.predict(len(pre_policy) + len(post_policy))\n",
    "\n",
    "\n",
    "    # # Create synthetic control group by combining predictions from control regions\n",
    "    # synthetic_control_group = np.mean(list(control_predictions.values()), axis=0)\n",
    "\n",
    "    # print(f\"SYNTHETIC LENGTH: {len(synthetic_control_group)}\")\n",
    "    # print(f\"{response_gdp}: {len(control_predictions)}\")\n",
    "\n",
    "    # data_pairs = zip(pre_policy['Year'], synthetic_control_group[:len(pre_policy)])\n",
    "\n",
    "    # for p in data_pairs:\n",
    "    #     print(p)\n",
    "\n",
    "\n",
    "    # # Plot post-policy data\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(pre_policy['Year'], treated_arima_model.predict(len(pre_policy)), label=f'{response_gdp} (Pre-policy)')\n",
    "    # plt.plot(pre_policy['Year'], synthetic_control_group[:len(pre_policy)], label='Synthetic Control (Pre-policy)')\n",
    "    # plt.plot(post_policy['Year'], treated_arima_model.predict(len(post_policy)), label=f'{\"Affected County\"} (Treated)')\n",
    "    # plt.plot(post_policy['Year'], synthetic_control_group[len(pre_policy):], label='Synthetic Control')\n",
    "    # # plt.plot(pre_policy['Year'], pre_policy[response_gdp], 'o', label='Actual Data (Pre-policy)')\n",
    "    # # plt.plot(post_policy['Year'], post_policy[response_gdp], 'o', label='Actual Data (Post-policy)')\n",
    "    # plt.axvline(route_attrition_year, color='red', linestyle='--', label='Policy Change Date')\n",
    "    # plt.xlabel('Year')\n",
    "    # plt.ylabel(response_gdp)\n",
    "    # plt.title('Effect of Policy Change on Local Economies')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # Plot post-policy data\n",
    "\n",
    "\n",
    "    # get the weighted averages of the others:\n",
    "\n",
    "    try:\n",
    "\n",
    "        name = fips_mapping_df[fips_mapping_df[\"fips_code\"].apply(custom_fips_code) == custom_fips_code(affected_fips_id)][\"name\"].values[0]\n",
    "    \n",
    "    except:\n",
    "\n",
    "        name = custom_fips_code(affected_fips_id)\n",
    "\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], label=f'{response_gdp} (Treated)')\n",
    "    # plt.plot(treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], pd.concat([total_input_data.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 -  pre : route_attrition_year - 2001], control_df.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 : route_attrition_year - 2001 + post]]), label='Synthetic Control')\n",
    "    # # plt.plot(post_policy['Year'], treated_arima_model.predict(len(post_policy)), label=f'{\"Affected County\"} (Treated)')\n",
    "    # # plt.plot(post_policy['Year'], synthetic_control_group[len(pre_policy):], label='Synthetic Control')\n",
    "    # # plt.plot(pre_policy['Year'], pre_policy[response_gdp], 'o', label='Actual Data (Pre-policy)')\n",
    "    # # plt.plot(post_policy['Year'], post_policy[response_gdp], 'o', label='Actual Data (Post-policy)')\n",
    "    # plt.axvline(route_attrition_year + (quarter/4) - (0.125), color='red', linestyle='--', label='Route Closure')\n",
    "    # plt.xlabel('Year')\n",
    "    # plt.ylabel(response_gdp)\n",
    "    # plt.title(f'Effect of Route Closure on {name} {response_gdp}')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    # print(fips_mapping_df[fips_mapping_df[\"mkt_id\"] == affected_fips_id][\"name\"])\n",
    "    \n",
    "    \n",
    "    # print(fips_mapping_df.loc[fips_mapping_df[\"mkt_id\"] == affected_fips_id, \"name\"].values[0])\n",
    "    \n",
    "    # # Plot results\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(post_policy['Year'], treated_arima_model.predict(len(post_policy)), label=f'{response_gdp} (Post-policy)')\n",
    "    # plt.plot(post_policy['Year'], synthetic_control_group[len(post_policy):], label='Synthetic Control (Post-policy)')\n",
    "    # plt.plot(post_policy['Year'], post_policy[response_gdp], 'o', label='Actual Data (Post-policy)')\n",
    "    # plt.axvline(route_attrition_year, color='red', linestyle='--', label='Policy Change Date')\n",
    "    # plt.xlabel('Year')\n",
    "    # plt.ylabel(response_gdp)\n",
    "    # plt.title('Effect of Policy Change on Local Economies')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # # PlYear\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# # synthetic_control_model(df, treated_region='TreatedRegion', control_regions=['ControlRegion1', 'ControlRegion2'],\n",
    "# #                         policy_change_date='2023-01-01', response_variable='GDP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # getting the treated region\n",
    "\n",
    "# route_ids = scm_dataframe_dict.keys()\n",
    "\n",
    "# for route in route_ids:\n",
    "\n",
    "#     for county_code in scm_dataframe_dict[route].keys():\n",
    "\n",
    "#         # for desc in [\"Utilities\", \"Agriculture, forestry, fishing and hunting\", \"Administrative and support and waste management and remediation services\", \"Professional, scientific, and technical services\", \"Private goods-producing industries 2/\", \"Durable goods manufacturing\", \"Finance and insurance\", \"Management of companies and enterprises\", \"Educational services\", \"Manufacturing\", \"Professional and business services\", \"Arts, entertainment, and recreation\", \"Mining, quarrying, and oil and gas extraction\"]:\n",
    "#         # for desc in [\"Utilities\", \"Agriculture, forestry, fishing and hunting\", \"Administrative and support and waste management and remediation services\", \"Professional, scientific, and technical services\", \"Private goods-producing industries 2/\", \"Durable goods manufacturing\"]:\n",
    "#         for desc in [\"Utilities\", \"Management of companies and enterprises\", \"Manufacturing\", \"Professional and business services\", \"Arts, entertainment, and recreation\", \"Durable goods manufacturing\"]:\n",
    "\n",
    "#             try:\n",
    "                \n",
    "#                 quarter = temp_shutdown_df[temp_shutdown_df[\"citadel_id\"] == route][\"quarter\"].values[-1]\n",
    "\n",
    "#                 affected_fips_id = scm_dataframe_dict[route][county_code].loc[scm_dataframe_dict[route][county_code][\"affected\"] == True, \"GeoFIPS\"].values[0]\n",
    "#                 # print(f\"ID WE ARE CONSIDERING: {affected_fips_id}\")\n",
    "#                 if route_and_affected_fips[route][1] != 2008 and route_and_affected_fips[route][1] != 2009:\n",
    "                    \n",
    "#                     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 2, quarter)\n",
    "#                     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 3, quarter)\n",
    "#                     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 4, quarter)\n",
    "                \n",
    "#             except:\n",
    "\n",
    "#                 pass\n",
    "\n",
    "\n",
    "# # control_pool = scm_dataframe_dict[3150332467][\"agaad\"][\"GeoFIPS\"].unique()\n",
    "\n",
    "# # dataset = synthetic_control_model(scm_dataframe_dict[3150332467][\"agaad\"], route_and_affected_fips[3150332467][1],  \"All industry total\", affected_fips_id)\n",
    "\n",
    "# # calif_synth_lr = (dataset.query(\"~target\")\n",
    "# #                   .pivot(index='Year', columns=\"cust_id\")[\"All industry total\"]\n",
    "# #                   .values.dot(weights_lr))\n",
    "\n",
    "# # sinthetic_states = [synthetic_control_model(scm_dataframe_dict[3150332467][\"agaad\"], route_and_affected_fips[3150332467][1], \"All industry total\", control_id) for control_id in control_pool]\n",
    "\n",
    "# # plt.figure(figsize=(12,7))\n",
    "# # for state in sinthetic_states:\n",
    "# #     plt.plot(state[\"Year\"], state[\"All industry total\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n",
    "\n",
    "# # plt.plot(scm_dataframe_dict[3150332467][\"agaad\"].query(\"target\")[\"Year\"], scm_dataframe_dict[3150332467][\"agaad\"].query(\"california\")[\"cigsale\"] - calif_synth_lr,\n",
    "# #         label=\"California\");\n",
    "\n",
    "# # plt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\n",
    "# # plt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\n",
    "# # plt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\n",
    "# # plt.title(\"State - Synthetic Across Time\")\n",
    "# # plt.legend()\n",
    "\n",
    "# # # , route_attrition_year, response_gdp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%pip install dill\n",
    "\n",
    "import dill\n",
    "dill.dump_session('/Users/tristanbrigham/Desktop/notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pct_chng_df)\n",
    "import dill\n",
    "dill.load_session('/Users/tristanbrigham/Desktop/notebook_env.db')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# control_pool = scm_dataframe_dict[3150332467][\"agaad\"][\"GeoFIPS\"].unique()\n",
    "\n",
    "# dataset = synthetic_control_model(scm_dataframe_dict[3150332467][\"agaad\"], route_and_affected_fips[3150332467][1],  \"All industry total\", affected_fips_id)\n",
    "\n",
    "# calif_synth_lr = (dataset.query(\"~target\")\n",
    "#                   .pivot(index='Year', columns=\"cust_id\")[\"All industry total\"]\n",
    "#                   .values.dot(weights_lr))\n",
    "\n",
    "# sinthetic_states = [synthetic_control_model(scm_dataframe_dict[3150332467][\"agaad\"], route_and_affected_fips[3150332467][1], \"All industry total\", control_id) for control_id in control_pool]\n",
    "\n",
    "# plt.figure(figsize=(12,7))\n",
    "# for state in sinthetic_states:\n",
    "#     plt.plot(state[\"Year\"], state[\"All industry total\"] - state[\"synthetic\"], color=\"C5\",alpha=0.4)\n",
    "\n",
    "# plt.plot(scm_dataframe_dict[3150332467][\"agaad\"].query(\"target\")[\"Year\"], scm_dataframe_dict[3150332467][\"agaad\"].query(\"california\")[\"cigsale\"] - calif_synth_lr,\n",
    "#         label=\"California\");\n",
    "\n",
    "# plt.vlines(x=1988, ymin=-50, ymax=120, linestyle=\":\", lw=2, label=\"Proposition 99\")\n",
    "# plt.hlines(y=0, xmin=1970, xmax=2000, lw=3)\n",
    "# plt.ylabel(\"Gap in per-capita cigarette sales (in packs)\")\n",
    "# plt.title(\"State - Synthetic Across Time\")\n",
    "# plt.legend()\n",
    "\n",
    "# # , route_attrition_year, response_gdp\n",
    "import statsmodels.api as sm\n",
    "def new_avg_SCM(pct_chng_df, route_attrition_year, response_gdp, affected_fips_id, pre, post, quarter):\n",
    "\n",
    "    # we need to transpose the matrix that we are given to start\n",
    "    # that way the description gdp is one of the columns\n",
    "    # and the years are the indexes\n",
    "\n",
    "    temp_cols = [\"Description\"]\n",
    "    \n",
    "    for year_col in total_year_cols:\n",
    "        temp_cols.append(year_col)\n",
    "\n",
    "\n",
    "\n",
    "    # get rid of any data that has a changed route\n",
    "    arr_1 = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(affected_fips_id)]\n",
    "    arr_2 = pct_chng_df[pct_chng_df[\"route_gone\"] == False]\n",
    "\n",
    "    pct_chng_df = pd.concat([arr_1, arr_2]).reset_index(drop=True)\n",
    "\n",
    "    minimum_value = pct_chng_df[pct_chng_df[\"GeoFIPS\"] != affected_fips_id]['mse_score'].min()\n",
    "    min_fips = pct_chng_df[pct_chng_df[\"mse_score\"] == minimum_value]['GeoFIPS'].values[0]\n",
    "\n",
    "\n",
    "    # we are going to go index by index, transpose the matrix, and add the other columns back in as necessary\n",
    "    fips_ids = pct_chng_df[\"GeoFIPS\"].unique()\n",
    "    # fips_ids = [affected_fips_id, min_fips]\n",
    "\n",
    "    # df that we are going to be writing to\n",
    "    total_input_data = pd.DataFrame(columns=[\"Year\", response_gdp, \"GeoFIPS\", \"cust_id\"])\n",
    "\n",
    "    # go through each of the fips ids and get the data that is associated with them\n",
    "    for fips_id in fips_ids:\n",
    "\n",
    "        # get the data that is associated with that fips id\n",
    "        temp_fips_df = pct_chng_df[pct_chng_df[\"cust_id\"] == custom_fips_code(fips_id)]\n",
    "\n",
    "        # get the transpose of the matrix\n",
    "        temp_fips_df = temp_fips_df[temp_cols]\n",
    "        temp_fips_df = temp_fips_df.T\n",
    "\n",
    "        # set the new column headers\n",
    "        new_column_headers = temp_fips_df.iloc[0]\n",
    "        temp_fips_df = temp_fips_df[1:]\n",
    "\n",
    "\n",
    "        # Set the new column headers and rename the axis\n",
    "        temp_fips_df.columns = new_column_headers\n",
    "        temp_fips_df = temp_fips_df.rename_axis(None, axis=1)\n",
    "\n",
    "        # rename and get only the interesting data\n",
    "        temp_fips_df = temp_fips_df[response_gdp].reset_index().rename(columns = {\"index\" : \"Year\"})\n",
    "\n",
    "        # add the ids\n",
    "        temp_fips_df[\"GeoFIPS\"] = str(fips_id)\n",
    "        temp_fips_df[\"cust_id\"] = str(custom_fips_code(fips_id))\n",
    "\n",
    "        total_input_data = pd.concat([total_input_data, temp_fips_df])\n",
    "\n",
    "        \n",
    "    # go ahead and adjust the data frame\n",
    "    total_input_data[\"Year\"] = total_input_data[\"Year\"].apply(pd.to_numeric, errors='coerce')\n",
    "    total_input_data[\"target\"] = total_input_data[\"cust_id\"] == custom_fips_code(affected_fips_id)\n",
    "    total_input_data[\"after_treatment\"] = total_input_data[\"Year\"] >= int(route_attrition_year)\n",
    "\n",
    "    # plt.figure(figsize=(10,6))\n",
    "    # plt.plot(total_input_data.query(\"target\")[\"Year\"], total_input_data.query(\"target\")[response_gdp], label=\"Target\")\n",
    "    # plt.plot(total_input_data.query(\"target\")[\"Year\"], routes_synth_lr, label=\"Synthetic Control\")\n",
    "    # plt.vlines(x=route_attrition_year, ymin=-1, ymax=1, linestyle=\":\", lw=2, label=\"Route Attrition\")\n",
    "    # plt.ylabel(\"GDP Growth Over Years\")\n",
    "    # plt.legend()\n",
    "\n",
    "    sc = Synth(total_input_data, response_gdp, \"GeoFIPS\", \"Year\", route_attrition_year, affected_fips_id, n_optim=100)\n",
    "    sc.plot([\"original\", \"pointwise\", \"cumulative\"], treated_label=affected_fips_id, synth_label=\"Synthetic County\", treatment_label=\"Route Closure\")\n",
    "\n",
    "\n",
    "    treated_df = total_input_data[total_input_data['cust_id'] == custom_fips_code(affected_fips_id)]\n",
    "    control_df = total_input_data[~(total_input_data['cust_id'] == custom_fips_code(affected_fips_id))]\n",
    "\n",
    "\n",
    "    # Separate pre- and post-policy data\n",
    "    pre_policy = total_input_data[total_input_data['Year'] < int(route_attrition_year)]\n",
    "    post_policy = total_input_data[total_input_data['Year'] >= int(route_attrition_year)]\n",
    "\n",
    "\n",
    "    Fit ARIMA model for each control region\n",
    "    # control_arima_models = {}\n",
    "    print(F\"AFFECTED REGIONS: {affected_fips_id}\")\n",
    "    print(F\"CONTROL REGIONS: {list(set(fips_ids))}\")\n",
    "\n",
    "\n",
    "    # go through each of the data sets\n",
    "    for untouched_id in list(set(fips_ids)):\n",
    "\n",
    "        if untouched_id == affected_fips_id:\n",
    "            continue\n",
    "\n",
    "        # get untouched regions\n",
    "        region_df = control_df[control_df['cust_id'] == custom_fips_code(untouched_id)]\n",
    "\n",
    "        # gridsearch for the model\n",
    "        control_arima_models[untouched_id] = auto_arima(region_df[response_gdp], seasonal=False)\n",
    "\n",
    "\n",
    "    # Fit ARIMA model for the treated region\n",
    "    treated_arima_model = auto_arima(treated_df[response_gdp], seasonal=False)\n",
    "\n",
    "\n",
    "    # Predict using ARIMA models for control regions\n",
    "    control_predictions = {}\n",
    "    for region, model in control_arima_models.items():\n",
    "\n",
    "        control_predictions[region] = model.predict(len(pre_policy) + len(post_policy))\n",
    "\n",
    "\n",
    "    # Create synthetic control group by combining predictions from control regions\n",
    "    synthetic_control_group = np.mean(list(control_predictions.values()), axis=0)\n",
    "\n",
    "    print(f\"SYNTHETIC LENGTH: {len(synthetic_control_group)}\")\n",
    "    print(f\"{response_gdp}: {len(control_predictions)}\")\n",
    "\n",
    "    data_pairs = zip(pre_policy['Year'], synthetic_control_group[:len(pre_policy)])\n",
    "\n",
    "    for p in data_pairs:\n",
    "        print(p)\n",
    "\n",
    "\n",
    "    # # Plot post-policy data\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(pre_policy['Year'], treated_arima_model.predict(len(pre_policy)), label=f'{response_gdp} (Pre-policy)')\n",
    "    # plt.plot(pre_policy['Year'], synthetic_control_group[:len(pre_policy)], label='Synthetic Control (Pre-policy)')\n",
    "    # plt.plot(post_policy['Year'], treated_arima_model.predict(len(post_policy)), label=f'{\"Affected County\"} (Treated)')\n",
    "    # plt.plot(post_policy['Year'], synthetic_control_group[len(pre_policy):], label='Synthetic Control')\n",
    "    # # plt.plot(pre_policy['Year'], pre_policy[response_gdp], 'o', label='Actual Data (Pre-policy)')\n",
    "    # # plt.plot(post_policy['Year'], post_policy[response_gdp], 'o', label='Actual Data (Post-policy)')\n",
    "    # plt.axvline(route_attrition_year, color='red', linestyle='--', label='Policy Change Date')\n",
    "    # plt.xlabel('Year')\n",
    "    # plt.ylabel(response_gdp)\n",
    "    # plt.title('Effect of Policy Change on Local Economies')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    # Plot post-policy data\n",
    "\n",
    "    treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post]\n",
    "    \n",
    "    df_after_years = treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001]\n",
    "    df_before_years = treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001]\n",
    "\n",
    "    # df_before_data_treated = pd.DataFrame(treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001], columns=[\"gdp\"])\n",
    "    df_before_data_treated = treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001].to_frame()\n",
    "    df_before_data_treated['t'] = 0\n",
    "    df_before_data_treated['g'] = 1\n",
    "    df_before_data_treated.rename(columns={response_gdp : \"gdp\"}, inplace=True)\n",
    "    df_before_data_treated = df_before_data_treated.reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    # df_after_data_treated = pd.DataFrame(treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001], columns=[\"gdp\"])\n",
    "    df_after_data_treated = treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001].to_frame()\n",
    "    df_after_data_treated['t'] = 1\n",
    "    df_after_data_treated['g'] = 1\n",
    "    df_after_data_treated.rename(columns={response_gdp : \"gdp\"}, inplace=True)\n",
    "    df_after_data_treated = df_after_data_treated.reset_index(drop=True)\n",
    "\n",
    "    # df_before_normal = pd.DataFrame(total_input_data.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 -  pre : route_attrition_year - 2001], columns=[\"gdp\"])\n",
    "    df_before_normal = total_input_data.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 -  pre : route_attrition_year - 2001].to_frame()\n",
    "    df_before_normal['t'] = 0\n",
    "    df_before_normal['g'] = 0\n",
    "    df_before_normal.rename(columns={response_gdp : \"gdp\"}, inplace=True)\n",
    "    df_before_normal = df_before_normal.reset_index(drop=True)\n",
    "\n",
    "    # df_after_normal = pd.DataFrame(control_df.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 : route_attrition_year - 2001 + post], columns=[\"gdp\"])\n",
    "    df_after_normal = control_df.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 : route_attrition_year - 2001 + post].to_frame()\n",
    "    df_after_normal['t'] = 1\n",
    "    df_after_normal['g'] = 0\n",
    "    df_after_normal.rename(columns={response_gdp : \"gdp\"}, inplace=True)\n",
    "    df_after_normal = df_after_normal.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    # total_dat = pd.DataFrame(columns=['gdp', 't', 'g'])\n",
    "\n",
    "    total_dat = df_before_data_treated\n",
    "    total_dat = pd.concat([total_dat, df_after_data_treated])\n",
    "    total_dat = pd.concat([total_dat, df_before_normal])\n",
    "    total_dat = pd.concat([total_dat, df_after_normal])\n",
    "    # print(total_dat)\n",
    "\n",
    "    # create the interaction \n",
    "    total_dat['gt'] = total_dat.g * total_dat.t\n",
    "\n",
    "    total_dat[['g', 't', 'gt']] = total_dat[['g', 't', 'gt']].astype(float)\n",
    "    total_dat['gdp'] = pd.to_numeric(total_dat['gdp'], errors='coerce')\n",
    "    total_dat = total_dat.dropna(subset=['gdp'])\n",
    "\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    X = total_dat[['g', 't', 'gt']]\n",
    "    y = total_dat['gdp']\n",
    "\n",
    "    lr.fit(X, y)\n",
    "    # print(lr.coef_)  # the coefficient for gt is the DID, which is 2.75\n",
    "\n",
    "    from statsmodels.formula.api import ols\n",
    "    ols = sm.OLS(y, sm.add_constant(X)).fit()\n",
    "    # ols = ols('gdp ~ g + t + gt', data=total_dat).fit()\n",
    "    # print(ols.summary())\n",
    "    \n",
    "    \n",
    "    old_p_val = ols.pvalues['gt']\n",
    "    impact_score = lr.coef_[2]\n",
    "\n",
    "    if old_p_val < 0.2:\n",
    "\n",
    "        try:\n",
    "\n",
    "            name = fips_mapping_df[fips_mapping_df[\"fips_code\"].apply(custom_fips_code) == custom_fips_code(affected_fips_id)][\"name\"].values[0]\n",
    "        \n",
    "        except:\n",
    "\n",
    "            name = custom_fips_code(affected_fips_id)\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print()\n",
    "        print(ols.summary())\n",
    "        print(f\"P_VAL: {old_p_val}\")\n",
    "\n",
    "        # plt.figure(figsize=(10, 6))\n",
    "        # plt.plot(treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], treated_df[response_gdp][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], label=f'{response_gdp} (Treated)')\n",
    "        # plt.plot(treated_df['Year'][route_attrition_year - 2001 -  pre : route_attrition_year - 2001 + post], pd.concat([total_input_data.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 -  pre : route_attrition_year - 2001], control_df.groupby('Year')[response_gdp].mean()[route_attrition_year - 2001 : route_attrition_year - 2001 + post]]), label='Synthetic Control')\n",
    "        # # plt.plot(post_policy['Year'], treated_arima_model.predict(len(post_policy)), label=f'{\"Affected County\"} (Treated)')\n",
    "        # # plt.plot(post_policy['Year'], synthetic_control_group[len(pre_policy):], label='Synthetic Control')\n",
    "        # # plt.plot(pre_policy['Year'], pre_policy[response_gdp], 'o', label='Actual Data (Pre-policy)')\n",
    "        # # plt.plot(post_policy['Year'], post_policy[response_gdp], 'o', label='Actual Data (Post-policy)')\n",
    "        # plt.axvline(route_attrition_year + (quarter/4) - (0.125), color='red', linestyle='--', label='Route Closure')\n",
    "        # plt.xlabel('Year')\n",
    "        # plt.ylabel(response_gdp)\n",
    "        # plt.title(f'Effect of Route Closure on {name} {response_gdp}')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "\n",
    "    return old_p_val, impact_score\n",
    "\n",
    "\n",
    "# # getting the treated region\n",
    "\n",
    "route_ids = scm_dataframe_dict.keys()\n",
    "\n",
    "\n",
    "for desc in [\"Utilities\", \"Agriculture, forestry, fishing and hunting\", \"Administrative and support and waste management and remediation services\", \"Professional, scientific, and technical services\", \"Private goods-producing industries 2/\", \"Durable goods manufacturing\", \"Finance and insurance\", \"Management of companies and enterprises\", \"Educational services\", \"Manufacturing\", \"Professional and business services\", \"Arts, entertainment, and recreation\", \"Mining, quarrying, and oil and gas extraction\"]:\n",
    "# for desc in [\"Utilities\", \"Agriculture, forestry, fishing and hunting\", \"Administrative and support and waste management and remediation services\", \"Professional, scientific, and technical services\", \"Private goods-producing industries 2/\", \"Durable goods manufacturing\"]:\n",
    "# for desc in [\"Utilities\", \"Management of companies and enterprises\", \"Manufacturing\", \"Professional and business services\", \"Arts, entertainment, and recreation\", \"Durable goods manufacturing\"]:\n",
    "    \n",
    "    \n",
    "    for route in route_ids:\n",
    "\n",
    "        for county_code in scm_dataframe_dict[route].keys():\n",
    "\n",
    "            try:\n",
    "                \n",
    "                affected_fips_id = scm_dataframe_dict[route][county_code].loc[scm_dataframe_dict[route][county_code][\"affected\"] == True, \"GeoFIPS\"].values[0]\n",
    "                quarter = temp_shutdown_df[temp_shutdown_df[\"citadel_id\"] == route][\"quarter\"].values[-1]\n",
    "\n",
    "                name = fips_mapping_df[fips_mapping_df[\"fips_code\"].apply(custom_fips_code) == custom_fips_code(affected_fips_id)][\"name\"].values[0]\n",
    "\n",
    "                # if name == \"Amador\" and quarter == 1 and route_and_affected_fips[route][1] == 2016:\n",
    "                    # synthetic_control_model(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id)\n",
    "\n",
    "                for horizon in range(2, 4):\n",
    "                    \n",
    "                    p_val, did = new_avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, horizon, quarter)\n",
    "\n",
    "                    if p_val < 0.2 and route_and_affected_fips[route][1] != 2009:\n",
    "\n",
    "                        print(f\"{route} | {desc} | {horizon} | {did} | {p_val} | {name} | {quarter} | {affected_fips_id} | {route_and_affected_fips[route][1]}\")\n",
    "\n",
    "                        # avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 2, quarter)\n",
    "\n",
    "            # if name == \"Tuolumne\" and quarter == 4 and route_and_affected_fips[route][1] == 2006:\n",
    "            #     print()\n",
    "            #     print()\n",
    "            #     print()\n",
    "            #     print(f\"Arts, entertainment, and recreation: {route}\")\n",
    "            #     # synthetic_control_model(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id)\n",
    "            #     new_avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], \"Arts, entertainment, and recreation\", affected_fips_id, 5, 2, quarter)\n",
    "            #     print(route)\n",
    "            #     print()\n",
    "            #     print()\n",
    "\n",
    "            #     break\n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "            # # print(f\"ID WE ARE CONSIDERING: {affected_fips_id}\")\n",
    "            # if route_and_affected_fips[route][1] != 2008 and route_and_affected_fips[route][1] != 2009:\n",
    "                \n",
    "            #     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 2, quarter)\n",
    "            #     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 3, quarter)\n",
    "            #     avg_SCM(scm_dataframe_dict[route][county_code], route_and_affected_fips[route][1], desc, affected_fips_id, 5, 4, quarter)\n",
    "\n",
    "                \n",
    "            except:\n",
    "\n",
    "                pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amount and Type of Delays Leading Up to Airport Closure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at year vs. ratio closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amount and Type of Delays Leading Up to Airport Closure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "looking at year vs. ratio closures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is no correlation \n",
    "\n",
    "# Economic Growth over years vs. Exit of Airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Airlines Cut Back on Service After Storms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
