{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01a11e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.parser import parse\n",
    "\n",
    "from multiprocessing import Process, Manager, Pool, cpu_count\n",
    "from collections import Counter\n",
    "\n",
    "from proc_titles_function import my_function_wrapper\n",
    "\n",
    "# how many days after the end of a natural disaster to consider an article hit\n",
    "days_to_add = 60\n",
    "days_to_sub = 50\n",
    "random_year = 2025\n",
    "\n",
    "# buckets for year quarters\n",
    "quarter_starts = []\n",
    "quarter_starts.append(datetime.datetime(random_year, 1, 1))\n",
    "quarter_starts.append(datetime.datetime(random_year, 4, 1))\n",
    "quarter_starts.append(datetime.datetime(random_year, 7, 1))\n",
    "quarter_starts.append(datetime.datetime(random_year, 10, 1))\n",
    "quarter_starts.append(datetime.datetime(random_year, 12, 31))\n",
    "\n",
    "##\n",
    "\n",
    "# notes:\n",
    "\n",
    "# bbc needs to have custom scraper for news titles\n",
    "# bloomberg: a few in 3rd index of split /\n",
    "# cnbc: .html in link\n",
    "# daily mail: need to make sure that there is a date for the articles, same as cnbc\n",
    "# huffpost: TBD\n",
    "# foxnews: done\n",
    "# nbc: NEED TO GET TITLES FOR \"ID\" ARTICLES\n",
    "\n",
    "# daily star?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dccddd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_news = \"/Users/tristanbrigham/Desktop/Self Study/Computer Science/News Scraper/News Sites Folders/\"\n",
    "path_to_article_titles = \"/Users/tristanbrigham/Desktop/test_news/Articles_titles\"\n",
    "col_headers = [\"link\", \"date\"]\n",
    "\n",
    "# format: \n",
    "# headline delimit character\n",
    "# idx of check phrase\n",
    "# interesting words (spaced by '|') -get_article_links- if this is empty then take all\n",
    "# date format of link if it exists\n",
    "link_dict = {\n",
    "    \"bloomberg\" : ('-', [\"opinion\", \"news\", \"en\", \"features\", \"politics\"], \"y-m-d\") ,\n",
    "    \"cnbc\" : ('-', [\"html\"], \"y/m/d\") ,\n",
    "    \"dailymail\" :  ('-', [\"html\"], \"\") ,\n",
    "    \"foxnews\" :  (\"-\", [\"weather\", \"area-51\", \"official-polls\", \"austin\", \"apps-products-ios\", \"tech\", \"fox-and-friends\", \"family\", \"fox-friends\", \"faith-values\", \"great-outdoors\", \"slack-success\", \"health\", \"lifestyle\", \"miami\", \"midterms-2018\", \"slack-app\", \"nyc\", \"newsletters\", \"about\", \"compliance\", \"auto\", \"politics\", \"story\", \"donotsell\", \"shows\", \"food-drink\", \"real-estate\", \"travel\", \"updates\", \"gowatch\", \"recordkeeping\", \"transcript\", \"cw-the-real-texas-rangers\", \"instant-access\", \"person\", \"whats-changed\", \"newsletters-coronavirus\", \"chicago\", \"opinion\", \"science\", \"sports\", \"uncategorized\", \"sunday-morning-futures\", \"world\", \"fox-news\", \"story\", \"us\", \"entertainment\"], \"\") ,\n",
    "    \"nypost\" : (\"-\", '', \"y/m/d\") ,\n",
    "    \"thedailybeast\" : ('-', \"\", \"\") ,\n",
    "    \"theglobeandmail\" : (\"-\", \"\", \"\") ,\n",
    "    \"usatoday\" : ('-', \"\", 'y/m/d') ,\n",
    "    \"cnn\" :  ('-', [\"html\"], 'y/m/d') ,\n",
    "    \"bbc\" :  ('_', \"\", 'y/m/d') ,\n",
    "    \"nytimes\" : ('-', ['html'], \"y/m/d\") ,\n",
    "    \"washingtonpost\" : ('-', '', \"\") ,\n",
    "    \"dailystar\" : ('-', '', \"\") ,\n",
    "    \"huffpost\" : ('-', [\"entry\"], \"\") ,\n",
    "    \n",
    "\n",
    "    # fix id issue\n",
    "    # \"nbcnews\" : (\"-\", 3, '/', [\"news\", \"globalcitizen\", \"think\", \"dateline\", \"feature\", \"health\", \"investigations\", \"meet-the-press\", \"nbc-out\", \"nightly-news\", \"politics\", \"pop-culture\", \"science\", \"space\", \"tech\", \"tornadoes\", \"western-wildfires\", \"world\", \"msnbc\", \"media\", \"data-graphics\", \"author\", \"slideshow\", \"information\", \"storyline\", \"nbc-learn\", \"businessmain\", \"healthmain\", \"_news\", \"sciencemain\", \"technolog\", \"test\", \"usnews\", \"worldmain\",] \"\") ,\n",
    "\n",
    "    \n",
    "    # perma-block\n",
    "    # \"usnews\" : ('-', 0, '', \"\") ,\n",
    "    # \"cbsnews\" :  ('_', -1, '.', \"news\") ,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "path_to_news = \"/Users/tristanbrigham/Desktop/Self Study/Computer Science/News Scraper/News Sites Folders/\"\n",
    "path_to_article_titles = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Articles_titles/\"\n",
    "col_headers = [\"link\", \"date\"]\n",
    "\n",
    "# define a function to open the csv and then return the dataframe\n",
    "def get_article_links(name):\n",
    "\n",
    "    links_df = pd.read_csv(path_to_news + name + \"/news_articles.csv\", header=None, index_col=False)\n",
    "    links_df.columns = col_headers\n",
    "\n",
    "    # print(links_df.head(5))\n",
    "\n",
    "    return links_df\n",
    "\n",
    "\n",
    "# this is meant to parse the link and try to find a date in the case that the article doesn't have one\n",
    "def get_date(link, args):\n",
    "\n",
    "    try:\n",
    "\n",
    "        date_format = args[3]\n",
    "\n",
    "        if date_format == \"\":\n",
    "\n",
    "            return \"\"\n",
    "        \n",
    "        # now we can assume that there is a date format\n",
    "        # search the link for a date (only considering 2000 and beyond)\n",
    "        date_idx = link.index(\"20\")\n",
    "\n",
    "        # check if this is a random 20 or not\n",
    "        if link[date_idx + 4 != args[3][1]]:\n",
    "\n",
    "            return \"\"\n",
    "\n",
    "        # assume that this is not a random 20\n",
    "        date = link[date_idx : date_idx + 10]\n",
    "\n",
    "        return date\n",
    "\n",
    "    except:\n",
    "\n",
    "        # print(f\"Couldn't find date: {link}\")\n",
    "        return \"\"\n",
    "    \n",
    "\n",
    "# this function takes the title and splits it based off of the arguments provided\n",
    "def split_title(link, args):\n",
    "\n",
    "    out = link.split(args[0])\n",
    "\n",
    "    out = \" \".join(out).replace(\"https://www.cnbc.com/\", \"\").replace(\".html\", \"\").replace(\"https://www.bloomberg.com/\", \"\").replace(\"https://www.theglobeandmail.com/\", \"\").replace(\"https://www.thedailybeast.com/\", \"\").replace(\"https://nypost.com/\", \"\").replace(\"https://www.foxnews.com/\", \"\").replace(\"https://www.bbc.com/\", \"\").replace(\"https://www.dailymail.co.uk/\", \"\").replace(\"edition.\", \"\").replace(\"https://cnn.com/\", \"\").replace(\"https://www.usatoday.com/\", \"\").replace(\"https://www.huffingtonpost.com/\", \"\").replace(\"https://www.washingtonpost.com/\", \"\").replace(\"https://www.nytimes.com/\", \"\").replace(\"https://www.dailystar.co.uk/\", \"\")\n",
    "\n",
    "    out = out.split(\"/\")\n",
    "\n",
    "    return \" \".join(out)\n",
    "\n",
    "\n",
    "def count_words_in_string(word_array, target_string):\n",
    "    common_words = [word for word in word_array if word in target_string]\n",
    "    return len(common_words)\n",
    "\n",
    "\n",
    "# this function checks whether the title of the news article that it is passed has any of the words that might indicate weather event\n",
    "# this is done in two ways -- first, we are looking for any words that are in the list that we have passed it below\n",
    "# second, we are looking for the names of any storms \n",
    "def check_weather(link):\n",
    "\n",
    "    weather_words = ['storm', 'hurricane', 'typhoon', 'tornado', 'cyclone', 'thunderstorm', 'flood', 'drought', \n",
    "    'heat', 'cold', 'snowstorm', 'blizzard', 'hailstorm', 'monsoon', 'rainfall', 'snowfall', 'temperature', \n",
    "    'earthquake', 'tsunami', 'volcano', 'wildfire', 'landslide', 'avalanche', 'eruption', 'seismic', 'environment', \n",
    "    'climate change', 'biodiversity', 'deforestation', 'ozone', 'sunny', 'cloudy',\n",
    "    'windy', 'rainy', 'snowy', 'overcast', 'humid', 'foggy', 'aurora', 'eclipse', 'meteor', 'comet', 'solar', \n",
    "    'flare', 'lightning', 'precipitat', 'drizzle', 'sleet', 'muggy', 'breezy', 'gust', 'heat', 'wind', 'dew', \n",
    "    'barometer', 'thermometer', 'anemometer', 'forecast', 'atmospheric', 'pressure', 'uv', 'index', \n",
    "    'seismology', 'aftershock', 'fault line', 'evacuation', 'emergency', 'disaster', 'relief', 'catastrophe', \n",
    "    'resilience', 'preparedness', 'shelter', 'disaster recovery', 'warm front', 'cold front', 'weather system', \n",
    "    'isobar', 'microclimate', 'atmospheric conditions', 'weather patterns', 'climate patterns', \n",
    "    'aurora borealis', 'northern lights', 'aurora australis', 'southern lights', 'meteor shower', \n",
    "    'solar', 'eclipse', 'lunar', 'eclipse', 'gravity wave', 'cosmic', 'rays', 'coronal mass ejection', 'snowfall', \n",
    "    'snowstorm', 'blizzard', 'snowdrift', 'snowflake', 'frost', 'ice', 'snowbank', 'slush', 'snowmelt', \n",
    "    'hail', 'winter', 'frosty', 'winter wonderland', 'polar vortex', 'snow', 'snow-capped', 'avalanche', 'whiteout', \n",
    "    'rainfall', 'showers', 'downpour', 'drizzle', 'cloudburst', 'deluge', 'rainstorm', 'raindrops', 'precipitation', \n",
    "    'thunderstorm', 'monsoon', 'rainy', 'wet', 'rain', 'rain gauge', 'rain cloud', 'drenching rain', 'rain shower', \n",
    "    'rainy season', 'rain boots', 'raincoat', 'windy', 'breezy', 'gust', 'gale', 'zephyr', 'bluster', 'whirlwind', \n",
    "    'cyclone', 'tornado', 'twister', 'tempest', 'windstorm', 'wind chill', 'windy weather', 'anemometer', 'wind', \n",
    "    'wind speed', 'wind', 'farm', 'wind', 'energy', 'system', 'pattern', 'atmospheric', 'condition', \n",
    "    'forecast', 'meteorology', 'front', 'weather']\n",
    "\n",
    "    # now check if the name of any storms exists\n",
    "    storm_words = [\n",
    "        'iniki', 'opal', 'sandy', 'imelda', 'isidore', 'michael', 'hugo', \n",
    "        'katrina', 'nicholas', 'storm', 'flood', 'irma', 'matthew', 'wildfire', 'elena', \n",
    "        'ian', 'harvey', 'allison', 'georgia', 'alberto', 'ike', 'blizzard', 'floods', 'isaac', 'allen', \n",
    "        'tropical', 'hanna', 'fire', 'cold', 'dorian', 'storm', \n",
    "        'hail', 'ice', 'jeanne', 'blizzard', 'hurricane', 'dolly', 'fiona', 'laura', 'delta', \n",
    "        'maria', 'isabel', 'lee', 'midwest/ohio', 'isaias', 'kansas', 'irene', 'wave', 'england', 'storm,', \n",
    "        'drought', 'heat', 'ivan', 'charley', 'floyd', 'ida', 'drought', 'andrew', 'flooding', 'gustav', \n",
    "        'erin', 'elsa', 'dennis', 'nicole', 'zeta', 'fran', 'heat', 'lili', 'georges', 'tornado', 'midwest', \n",
    "        'fred', 'juan', 'wilma', 'derecho,', 'marilyn', 'rita', 'sally', 'bob', 'firestorm', 'eta', 'gloria', \n",
    "        'derecho', 'winter', 'wildfires', 'alicia', 'freeze', 'bonnie', 'flash'\n",
    "        ]\n",
    "\n",
    "    \n",
    "    # for word in storm_words:\n",
    "\n",
    "    #     if word.lower() in link.lower():\n",
    "\n",
    "            # print(f\"STORM: {word}\")\n",
    "\n",
    "    # print(f\"storm: {any(word.lower() in link.lower() for word in storm_words)}\")\n",
    "    # print(f\"weather: {any(word.lower() in link.lower() for word in weather_words)}\")\n",
    "\n",
    "    pattern = r\"[!@#$%^&*()\\-_=+[{\\]};:'\\\",<.>/?\\\\|`~]\"\n",
    "\n",
    "    # Use re.sub() to replace the special characters with spaces\n",
    "    link = re.sub(pattern, ' ', link)\n",
    "\n",
    "    return any(word in [x.lower() for x in link.split(\" \")] for word in storm_words) or any(word in [x.lower() for x in link.split(\" \")] for word in weather_words)\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "def get_type_disaster(link):\n",
    "\n",
    "    pattern = r\"[!@#$%^&*()\\-_=+[{\\]};:'\\\",<.>/?\\\\|`~]\"\n",
    "\n",
    "    # Use re.sub() to replace the special characters with spaces\n",
    "    link = re.sub(pattern, ' ', link)\n",
    "\n",
    "    types_disasters = [\"Drought\", \"Cold\", \"Storm\", \"Fire\", \"Flood\", \"Cyclone\"]\n",
    "\n",
    "    drought_dict = ['drought', 'arid', 'dry', 'thirst', 'parched', 'waterless', 'scorching', 'heatwave', 'dehydrated', 'desiccated', 'dusty', 'cracked', 'barren', 'hot', 'thirsty', 'withered', 'deficit', 'scorched', 'rainless', 'shrinking', 'severe', 'lack', 'aridity', 'desert', 'water shortage', 'scarcity', 'heat', 'crop failure', 'dryness', 'famine', 'wildfire', 'shortage', 'water crisis', 'water stress', 'water rationing', 'waterless', 'dry spell', 'dustbowl', 'crop loss', 'thirsty crops', 'parch', 'crop damage', 'drying', 'water conservation', 'rainfall deficiency', 'desertification', 'heat stress', 'reservoir levels', 'water depletion', 'hydrological drought']\n",
    "    cold_storm_dict = ['snowstorm', 'blizzard', 'frosty', 'chills', 'icy', 'snowy', 'brr', 'snowflakes', 'freeze', 'flurries', 'sleet', 'slush', 'frost', 'icicles', 'polar', 'arctic', 'whiteout', 'cold', 'wintry', 'snowfall', 'winter', 'snowbound', 'snowpack', 'hail', 'drifts', 'freeze', 'winterize', 'snowmageddon', 'snowbanks', 'chill', 'frostbite', 'snowslide', 'hibernation', 'frigid', 'snowcap', 'snowmobile', 'snowshoes', 'snowplow', 'snowman', 'avalanche', 'powder', 'slippery', 'shiver', 'iciness', 'snowscape', 'snowbelt', 'snowball', 'winterize', 'snowy', 'cold snap']\n",
    "    other_storm_dict = [\"Storm\", \"Thunder\", \"Rain\", \"Flash\", \"Hail\", \"Wind\", \"Flood\", \"Clouds\", \"Lightning\", \"Snow\", \"Sleet\", \"Tornado\", \"Gales\", \"Downpour\", \"Squall\", \"Deluge\", \"Blustery\", \"Cyclone\", \"Tempest\", \"Drizzle\", \"Monsoon\", \"Cloudburst\", \"Whirlwind\", \"Typhoon\", \"Hurricane\", \"Drench\", \"Shower\", \"Torrent\", \"Wet\", \"Thunderclap\", \"Rainfall\", \"Snowfall\", \"Stormy\", \"Gusts\", \"Thunderhead\", \"Lightning strike\", \"Rumble\", \"Frost\", \"Mist\", \"Polar\", \"Gale-force\", \"Fog\", \"Twister\", \"Blizzard\", \"Snowflake\", \"Sleet\", \"Wetness\", \"Lightning bolt\", \"Flooded\", \"Cyclonic\",  ]\n",
    "    fire_dict = ['fire', 'blaze', 'inferno', 'flame', 'wildfire', 'burn', 'hot', 'smoke', 'heat', 'combustion', 'charred', 'ash', 'ember', 'scorch', 'smolder', 'ignite', 'flare', 'sear', 'scorched', 'incinerate', 'pyre', 'kindle', 'singe', 'conflagration', 'heatwave', 'flaming', 'fiery', 'spark', 'sizzle', 'blaze up', 'firestorm', 'burning', 'flameout', 'bonfire', 'incendiarism', 'flashfire', 'infernal', 'engulf', 'arson', 'firefighting', 'firefighter', 'blazing', 'ignition', 'fireball', 'fireproof', 'smoky', 'cinders', 'firetruck', 'combustible', 'flamingo']\n",
    "    flood_dict = ['flood', 'deluge', 'inundation', 'water', 'overflow', 'surge', 'submerge', 'swell', 'torrent', 'overflowing', 'rains', 'flash flood', 'waterlogged', 'dam', 'flooding', 'soaked', 'river', 'flooded', 'rising', 'evacuation', 'disaster', 'reservoir', 'emergency', 'levee', 'submerged', 'torrential', 'waterway', 'floodplain', 'floody', 'rainfall', 'floodwater', 'leaking', 'damaged', 'damaging', 'barrier', 'riverbank', 'inflow', 'deluged', 'floodgate', 'overflowed', 'flooded-out', 'rainstorm', 'water surge', 'overtop', 'soggy', 'flood-prone', 'heavy rain', 'flooded area', 'waterlogged', 'rainfall records']\n",
    "    cyclone_typhoon_dict = ['cyclone', 'typhoon', 'storm', 'wind', 'tropical', 'hurricane', 'gales', 'weather', 'dangerous', 'destruction', 'devastating', 'powerful', 'cyclonic', 'eye', 'landfall', 'intense', 'category', 'rain', 'evacuation', 'threat', 'warning', 'disaster', 'windy', 'severe', 'tornadoes', 'coastal', 'windswept', 'shelter', 'emergency', 'damaging', 'impact', 'prepare', 'safety', 'damage', 'landfall', 'surge', 'relief', 'resilience', 'evacuate', 'precautions', 'preparedness', 'cyclonic storm', 'typhoon warning', 'evacuation plan', 'weather alert', 'severe weather', 'cyclone aftermath', 'emergency response', 'tropical storm', \"cyclone's path\"]\n",
    "\n",
    "    temp_title = link\n",
    "    score_array = []\n",
    "\n",
    "    for l in [drought_dict, cold_storm_dict, other_storm_dict, fire_dict, flood_dict, cyclone_typhoon_dict]:\n",
    "\n",
    "        score_array.append(count_words_in_string(l, temp_title))\n",
    "\n",
    "\n",
    "    max_value = max(score_array)\n",
    "\n",
    "    if max_value == 0:\n",
    "\n",
    "        type_disaster = \"unknown\"\n",
    "\n",
    "    else:\n",
    "        \n",
    "        type_disaster = types_disasters[score_array.index(max_value)]\n",
    "\n",
    "    return type_disaster\n",
    "\n",
    "\n",
    "\n",
    "def split_date(date):\n",
    "\n",
    "    date = str(date)\n",
    "\n",
    "    year = date[:4]\n",
    "    month = date[4:6]\n",
    "    day = date[6:]\n",
    "\n",
    "    return str(year) + \"/\" + str(month) + \"/\" + str(day)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# checking if this date is in the right quarter (year agnostic)\n",
    "def check_quarter(date, idx):\n",
    "\n",
    "    return quarter_starts[idx] <= date.replace(year=random_year) <= quarter_starts[idx + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d0262a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new df for weather event mentions and then add that to the base one\n",
    "\n",
    "# def process_titles(weather_event_df):\n",
    "\n",
    "#     for key, information in link_dict.iterrows():\n",
    "    \n",
    "#         # key, information = next(iter(link_dict.items()))\n",
    "\n",
    "#         # go through each of the pairings... get the name of the news org and the respective news data first\n",
    "#         temp_df = get_article_links(key)\n",
    "\n",
    "#         # check if the right text is in the article title\n",
    "#         if information[2] != \"\":\n",
    "            \n",
    "#             # drop any of the rows that don't contain right text\n",
    "#             temp_df = temp_df[temp_df['link'].str.contains('|'.join(information[2]))]\n",
    "\n",
    "#         # now that we have the associated dataframe, go link by link and process it into individual words \n",
    "#         # that we can compare against our bad word dictionary\n",
    "#         # make sure that we keep the date there too\n",
    "\n",
    "#         # remember, we still need to process nbcnews'\n",
    "\n",
    "#         # temp_df[\"date\"] = temp_df[\"date\"].apply(lambda row: get_date(row[\"link\"], information) if row[\"date\"].isnull().all() else row)\n",
    "\n",
    "#         for _, row in temp_df.iterrows():\n",
    "\n",
    "#             # check if there is a date attached\n",
    "#             if pd.isna(row[\"date\"]):\n",
    "\n",
    "#                 row[\"date\"] = get_date(row[\"link\"], information)\n",
    "            \n",
    "#             # now we either have a date or we don't\n",
    "#             # if we don't, we can't consider this article link and should drop it\n",
    "\n",
    "#         temp_df = temp_df.dropna(subset=[\"date\"])\n",
    "#         temp_df = temp_df.loc[~(temp_df[\"date\"] == \"\")]\n",
    "\n",
    "#         ## WEATHER STUFF\n",
    "#         # check if the article mentions any weather events\n",
    "#         temp_df[\"mentions_weather\"] = temp_df[\"link\"].apply(check_weather)\n",
    "#         temp_df[\"weather_type\"] = None\n",
    "#         temp_df[\"weather_type\"] = temp_df[temp_df[\"mentions_weather\"]][\"link\"].apply(get_type_disaster)\n",
    "\n",
    "            \n",
    "#         # create a new df column that has the article names\n",
    "#         temp_df[\"title\"] = temp_df[\"link\"].apply(lambda age: split_title(age, information))\n",
    "\n",
    "\n",
    "#         # go line by line in the df now and increase the reference count for news-worthy weather events\n",
    "#         for _, men_row in temp_df[temp_df[\"mentions_weather\"] == True].iterrows():\n",
    "\n",
    "#             # parse the date of the article\n",
    "#             article_date = parse(men_row[\"date\"]).replace(tzinfo=None)\n",
    "\n",
    "#             # now add one to the reference count for weather events\n",
    "#             # for _, w_row in weather_event_df.iterrows():\n",
    "                \n",
    "#             weather_event_df.loc[(weather_event_df[\"start\"] < article_date) & (weather_event_df[\"end\"] + timedelta(days=days_to_add) > article_date), \"mentions\"] += 1\n",
    "#                 # weather_event_df[parse(weather_event_df[\"start\"]) < article_date & parse(weather_event_df[\"end\"]) + timedelta(days=days_to_add) > article_date, \"mentions\"] += 1\n",
    "            \n",
    "\n",
    "#         # save this to a new csv in my local citadel correlation folder\n",
    "#         temp_df.to_csv(path_to_article_titles + key + \"_titles.csv\", index=False)\n",
    "\n",
    "\n",
    "#         print(f\"DONE: {key}\")\n",
    "    \n",
    "#     return weather_event_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9685f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                name     disaster_type   \n",
      "0   Southern Severe Storms and Flooding (April 1980)          Flooding  \\\n",
      "1                      Hurricane Allen (August 1980)  Tropical Cyclone   \n",
      "2  Central/Eastern Drought/Heat Wave (Summer-Fall...           Drought   \n",
      "3                      Florida Freeze (January 1981)            Freeze   \n",
      "4  Severe Storms, Flash Floods, Hail, Tornadoes (...      Severe Storm   \n",
      "\n",
      "       start        end     cost  deaths  mentions     Q1     Q2     Q3   \n",
      "0 1980-04-10 1980-04-17   2650.4       7         0  False   True  False  \\\n",
      "1 1980-08-07 1980-08-11   2153.6      13         0  False  False   True   \n",
      "2 1980-06-01 1980-11-30  39178.2    1260         0  False   True  False   \n",
      "3 1981-01-12 1981-01-14   2002.0       0         0   True  False  False   \n",
      "4 1981-05-05 1981-05-10   1360.8      20         0  False   True  False   \n",
      "\n",
      "      Q4         str          nd  \n",
      "0  False  1980-02-20  1980-06-16  \n",
      "1  False  1980-06-18  1980-10-10  \n",
      "2   True  1980-04-12  1981-01-29  \n",
      "3  False  1980-11-23  1981-03-15  \n",
      "4  False  1981-03-16  1981-07-09  \n"
     ]
    }
   ],
   "source": [
    "# now we are going to go through each of the links in each of the news organzations\n",
    "# and parse the title of the article out from each of them\n",
    "\n",
    "# note that nbc news has a lot of titles that have \"id wbna\" in the title, so we are going to try to request those\n",
    "\n",
    "# get the path to the disaster data:\n",
    "path_to_disaster_data = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/Auxillary Data/NOAA Disaster Data/disasters_NOAA_events-US-1980-2023.csv\"\n",
    "\n",
    "# we are going to create a dataframe of storms now from the csv file that we have\n",
    "weather_event_df = pd.read_csv(path_to_disaster_data)\n",
    "\n",
    "# # changing the column names so that it is more intuitive\n",
    "# new_column_names = {\"Name\"  : 'name',\n",
    "#                     \"Disaster\" : 'disaster_type',\n",
    "#                     \"Begin Date\" : 'start',\n",
    "#                     \"End Date\" : 'end',\n",
    "#                     \"Total CPI-Adjusted Cost (Millions of Dollars)\" : 'cost',\n",
    "#                     \"Deaths\" : 'deaths'}\n",
    "\n",
    "# rename the columns\n",
    "# weather_event_df.rename(columns=new_column_names, inplace=True)\n",
    "\n",
    "# fix the data data so that it is reflective\n",
    "weather_event_df['start'] = weather_event_df['start'].apply(lambda x: parse(x).replace(tzinfo=None))\n",
    "weather_event_df['end'] = weather_event_df['end'].apply(lambda x: parse(x).replace(tzinfo=None))\n",
    "\n",
    "weather_event_df['mentions'] = 0\n",
    "\n",
    "print(weather_event_df.head(5))\n",
    "\n",
    "\n",
    "# now we have our weather data set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9c2211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING: bloomberg\n",
      "STARTING: dailymail\n",
      "STARTING: cnbc\n",
      "STARTING: thedailybeast\n",
      "STARTING: foxnews\n",
      "STARTING: usatoday\n",
      "STARTING: nypost\n",
      "STARTING: theglobeandmail\n",
      "STARTING: nytimes\n",
      "STARTING: cnn\n",
      "STARTING: bbc\n",
      "STARTING: washingtonpost\n",
      "STARTING: dailystar\n",
      "STARTING: huffpost\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39m# Create a pool of processes with the desired number of worker processes\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mwith\u001b[39;00m Pool(processes\u001b[39m=\u001b[39mnum_processes) \u001b[39mas\u001b[39;00m pool:\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m     \u001b[39m# Use the DataFrame rows as the iterable for parallel processing\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     results\u001b[39m.\u001b[39mappend(pool\u001b[39m.\u001b[39;49mmap(my_function_wrapper, input_data))\n\u001b[1;32m     16\u001b[0m \u001b[39m# Update the 'Result' column with the output of the function\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m results:\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/multiprocessing/pool.py:367\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    363\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[1;32m    364\u001b[0m \u001b[39m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, mapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_processes = cpu_count()  # Number of CPU cores available\n",
    "\n",
    "input_data = []\n",
    "results = []\n",
    "    \n",
    "for key, info in link_dict.items():\n",
    "\n",
    "    input_data.append((key, info, weather_event_df.copy()))\n",
    "\n",
    "# Create a pool of processes with the desired number of worker processes\n",
    "with Pool(processes=num_processes) as pool:\n",
    "\n",
    "    # Use the DataFrame rows as the iterable for parallel processing\n",
    "    results.append(pool.map(my_function_wrapper, input_data))\n",
    "\n",
    "# Update the 'Result' column with the output of the function\n",
    "for df in results:\n",
    "\n",
    "    weather_event_df[\"mentions\"] += df[\"mentions\"]\n",
    "\n",
    "print(weather_event_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05ef401",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weather_event_df[\"Q1\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 0))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 0))\n",
    "weather_event_df[\"Q2\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 1))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 1))\n",
    "weather_event_df[\"Q3\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 2))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 2))\n",
    "weather_event_df[\"Q4\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 3))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 3))\n",
    "# weather_event_df[\"Q1\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 0))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 0))\n",
    "# weather_event_df[\"Q2\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 1))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 1))\n",
    "# weather_event_df[\"Q3\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 2))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 2))\n",
    "# weather_event_df[\"Q4\"] = weather_event_df[\"start\"].apply((lambda date: check_quarter(date, 3))) | weather_event_df[\"end\"].apply(lambda date: check_quarter(date, 3))\n",
    "\n",
    "# see if you can add some propogation to this? Like a flag that says prev quarter wasn't awesome\n",
    "article_date = parse(\"2022/8/28\").replace(tzinfo=None)\n",
    "weather_event_df.loc[(weather_event_df[\"start\"] - timedelta(days=days_to_sub) < article_date) & (weather_event_df[\"end\"] + timedelta(days=days_to_add) > article_date), \"mentions\"] += 1\n",
    "weather_event_df[\"str\"] = weather_event_df[\"start\"] - timedelta(days=days_to_sub)\n",
    "weather_event_df[\"nd\"] = weather_event_df[\"end\"] + timedelta(days=days_to_add)\n",
    "\n",
    "weather_event_df.to_csv(path_to_disaster_data, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3d2c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # let's add quarters to the df\n",
    "# for idx, title in temp_df[temp_df[\"mentions_weather\"] == True].iterrows():\n",
    "\n",
    "#     print(title[\"title\"])\n",
    "#     print(check_weather(title[\"link\"]))\n",
    "\n",
    "#     if idx > 500:\n",
    "\n",
    "#         break\n",
    "\n",
    "# temp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6227273",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a06b592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
