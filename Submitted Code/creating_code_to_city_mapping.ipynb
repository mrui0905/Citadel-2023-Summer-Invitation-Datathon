{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/Local Important Data/Consumer_Airfare_Report__Table_6_-_Contiguous_State_City-Pair_Markets_That_Average_At_Least_10_Passengers_Per_Day.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m path_to_mkt_numbers \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/Local Important Data/Consumer_Airfare_Report__Table_6_-_Contiguous_State_City-Pair_Markets_That_Average_At_Least_10_Passengers_Per_Day.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m path_to_mapping \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/code_to_region_state_mapping.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 10\u001b[0m mkts \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(path_to_mkt_numbers)\n\u001b[1;32m     11\u001b[0m mapping_file \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(path_to_mapping)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1662\u001b[0m     f,\n\u001b[1;32m   1663\u001b[0m     mode,\n\u001b[1;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1670\u001b[0m )\n\u001b[1;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/Local Important Data/Consumer_Airfare_Report__Table_6_-_Contiguous_State_City-Pair_Markets_That_Average_At_Least_10_Passengers_Per_Day.csv'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "# from geopy.geocoders import Nominatim\n",
    "\n",
    "# going to be creating an excel file with regions, states, market numbers\n",
    "path_to_mkt_numbers = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Local Important Data/Consumer_Airfare_Report__Table_6_-_Contiguous_State_City-Pair_Markets_That_Average_At_Least_10_Passengers_Per_Day.csv\"\n",
    "path_to_mapping = \"/Users/tristanbrigham/Desktop/Citadel Datathon/Citadel_Correlation_One_Datathon/crucial_data_and_files/code_to_region_state_mapping.csv\"\n",
    "\n",
    "mkts = pd.read_csv(path_to_mkt_numbers)\n",
    "mapping_file = pd.read_csv(path_to_mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(city):\n",
    "    return city.split(\",\")[-1].replace(\"(Metropolitan Area)\", \"\").strip()\n",
    "\n",
    "def get_long(geo_name):\n",
    "    try:\n",
    "        geo_name = geo_name.strip()[geo_name.index(\"(\") + 1 : -1]\n",
    "        geo_name = geo_name[geo_name.index(\",\") + 1 : ]\n",
    "        return geo_name\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def get_lat(geo_name):\n",
    "    try:\n",
    "        geo_name = geo_name.replace(\"(Metropolitan Area)\", \"\").strip()[geo_name.index(\"(\") + 1: ]\n",
    "        geo_name = geo_name[ : geo_name.index(\",\")]\n",
    "        return geo_name\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "mkts.dropna(subset=[\"Geocoded_City1\"], inplace=True)\n",
    "mkts.dropna(subset=[\"Geocoded_City2\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Bristol/Johnson City/Kingsport, TN\\n(42.083299...\n",
       "1                Portland, ME\\n(45.516018, -122.681425)\n",
       "2             Fayetteville, AR\\n(36.061928, -94.160583)\n",
       "3     Greenville/Spartanburg, SC\\n(44.513288, -88.01...\n",
       "4                   Buffalo, NY\\n(39.945422, -78.64671)\n",
       "5               Fort Wayne, IN\\n(41.331128, -73.987189)\n",
       "6     Atlanta, GA (Metropolitan Area)\\n(33.748547, -...\n",
       "7              Indianapolis, IN\\n(39.76845, -86.156212)\n",
       "8             Grand Rapids, MI\\n(47.925705, -97.036068)\n",
       "9                  Ashland, WV\\n(38.475807, -82.646675)\n",
       "10          Corpus Christi, TX\\n(27.796416, -97.404131)\n",
       "11    San Francisco, CA (Metropolitan Area)\\n(37.780...\n",
       "12                  Dayton, OH\\n(39.760982, -84.192203)\n",
       "13     Beaumont/Port Arthur, TX\\n(30.08617, -94.102012)\n",
       "14                 Chicago, IL\\n(41.775002, -87.696388)\n",
       "15              Binghamton, NY\\n(42.098702, -75.912543)\n",
       "16               Milwaukee, WI\\n(43.041072, -87.909421)\n",
       "17    Los Angeles, CA (Metropolitan Area)\\n(34.05223...\n",
       "18                     Erie, PA\\n(34.1647, -114.300301)\n",
       "19              Des Moines, IA\\n(41.588822, -93.620309)\n",
       "20    San Francisco, CA (Metropolitan Area)\\n(37.780...\n",
       "21    Cleveland, OH (Metropolitan Area)\\n(41.505546,...\n",
       "22         Gulfport/Biloxi, MS\\n(30.369834, -89.091582)\n",
       "23    New York City, NY (Metropolitan Area)\\n(40.123...\n",
       "24              Huntsville, AL\\n(34.729538, -86.585283)\n",
       "25             Indianapolis, IN\\n(39.76845, -86.156212)\n",
       "26    Cedar Rapids/Iowa City, IA\\n(41.978122, -91.66...\n",
       "27              Harrisburg, PA\\n(40.259572, -76.881821)\n",
       "28               Portland, ME\\n(45.516018, -122.681425)\n",
       "29          Raleigh/Durham, NC\\n(42.673348, -72.179683)\n",
       "30               Charlotte, NC\\n(35.222936, -80.840161)\n",
       "31              Burlington, VT\\n(44.475949, -73.212481)\n",
       "32                 Denver, CO\\n(39.738453, -104.984853)\n",
       "33             Little Rock, AR\\n(34.748745, -92.275105)\n",
       "34        Dallas/Fort Worth, TX\\n(40.11086, -77.035636)\n",
       "35                 Madison, WI\\n(43.073926, -89.385244)\n",
       "36                 Detroit, MI\\n(42.332916, -83.047853)\n",
       "37                Columbus, OH\\n(39.962649, -82.996216)\n",
       "38                  Austin, TX\\n(30.264979, -97.746598)\n",
       "39    Boston, MA (Metropolitan Area)\\n(42.358894, -7...\n",
       "40    Mission/McAllen/Edinburg, TX\\n(43.153621, -93....\n",
       "41                 Memphis, TN\\n(35.143378, -90.052136)\n",
       "42               Melbourne, FL\\n(28.079931, -80.603516)\n",
       "43    Cleveland, OH (Metropolitan Area)\\n(41.505546,...\n",
       "44              Fort Wayne, IN\\n(41.331128, -73.987189)\n",
       "45                  Aspen, CO\\n(39.190665, -106.819201)\n",
       "46                  Austin, TX\\n(30.264979, -97.746598)\n",
       "47    Los Angeles, CA (Metropolitan Area)\\n(34.05223...\n",
       "48                     Erie, PA\\n(34.1647, -114.300301)\n",
       "49               Charleston, SC\\n(32.77647, -79.931027)\n",
       "Name: Geocoded_City1, dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mkts[\"state_1\"] = mkts['city1'].apply(get_state)\n",
    "mkts[\"state_2\"] = mkts['city2'].apply(get_state)\n",
    "\n",
    "mkts[\"long_1\"] = mkts['Geocoded_City1'].apply(get_long)\n",
    "mkts[\"lat_1\"] = mkts['Geocoded_City1'].apply(get_lat)\n",
    "\n",
    "mkts[\"long_2\"] = mkts['Geocoded_City2'].apply(get_long)\n",
    "mkts[\"lat_2\"] = mkts['Geocoded_City2'].apply(get_lat)\n",
    "\n",
    "\n",
    "# print(mkts.describe())\n",
    "mkts[\"Geocoded_City1\"].head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mapping = pd.DataFrame(columns=[\"state\", \"city\", \"market_id\", \"longitude\", \"latitude\"])\n",
    "mapping[\"state\"] = mkts[\"state_1\"]\n",
    "mapping[\"city\"] = mkts[\"city1\"]\n",
    "mapping[\"market_id\"] = mkts[\"citymarketid_1\"]\n",
    "mapping[\"longitude\"] = mkts[\"long_1\"]\n",
    "mapping[\"latitude\"] = mkts[\"lat_1\"]\n",
    "\n",
    "mapping_2 = pd.DataFrame(columns=[\"state\", \"city\", \"market_id\", \"longitude\", \"latitude\"])\n",
    "mapping_2[\"state\"] = mkts[\"state_2\"]\n",
    "mapping_2[\"city\"] = mkts[\"city2\"]\n",
    "mapping_2[\"market_id\"] = mkts[\"citymarketid_2\"]\n",
    "mapping_2[\"longitude\"] = mkts[\"long_2\"]\n",
    "mapping_2[\"latitude\"] = mkts[\"lat_2\"]\n",
    "\n",
    "mapping = pd.concat([mapping, mapping_2])\n",
    "\n",
    "mapping.drop_duplicates(inplace=True)\n",
    "\n",
    "mapping.to_csv(path_to_mapping, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Drought', 'Winter Storm', 'Freeze', 'Severe Storm', 'Wildfire', 'Flooding', 'Tropical Cyclone'}\n"
     ]
    }
   ],
   "source": [
    "dat = [\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Flooding\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Flooding\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Freeze\",\n",
    "\"Winter Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Wildfire\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Winter Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Flooding\",\n",
    "\"Wildfire\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Winter Storm\",\n",
    "\"Flooding\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Flooding\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Flooding\",\n",
    "\"Freeze\",\n",
    "\"Winter Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Wildfire\",\n",
    "\"Flooding\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Wildfire\",\n",
    "\"Freeze\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Winter Storm\",\n",
    "\"Flooding\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Winter Storm\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Freeze\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Winter Storm\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Wildfire\",\n",
    "\"Drought\",\n",
    "\"Flooding\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Drought\",\n",
    "\"Wildfire\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Tropical Cyclone\",\n",
    "\"Wildfire\",\n",
    "\"Winter Storm\",\n",
    "\"Drought\",\n",
    "\"Winter Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Flooding\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",\n",
    "\"Severe Storm\",]\n",
    "\n",
    "print(set(dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cyclone', 'typhoon', 'storm', 'wind', 'tropical', 'hurricane', 'gales', 'weather', 'dangerous', 'destruction', 'devastating', 'powerful', 'cyclonic', 'eye', 'landfall', 'intense', 'category', 'rain', 'evacuation', 'threat', 'warning', 'disaster', 'windy', 'severe', 'tornadoes', 'coastal', 'windswept', 'shelter', 'emergency', 'damaging', 'impact', 'prepare', 'safety', 'damage', 'landfall', 'surge', 'relief', 'resilience', 'evacuate', 'precautions', 'preparedness', 'cyclonic storm', 'typhoon warning', 'evacuation plan', 'weather alert', 'severe weather', 'cyclone aftermath', 'emergency response', 'tropical storm', \"cyclone's path\"]\n"
     ]
    }
   ],
   "source": [
    "drought_dict = [\"Cyclone\", \"Typhoon\", \"Storm\", \"Wind\", \"Tropical\", \"Hurricane\", \"Gales\", \"Weather\", \"Dangerous\", \"Destruction\", \"Devastating\", \"Powerful\", \"Cyclonic\", \"Eye\", \"Landfall\", \"Intense\", \"Category\", \"Rain\", \"Evacuation\", \"Threat\", \"Warning\", \"Disaster\", \"Windy\", \"Severe\", \"Tornadoes\", \"Coastal\", \"Windswept\", \"Shelter\", \"Emergency\", \"Damaging\", \"Impact\", \"Prepare\", \"Safety\", \"Damage\", \"Landfall\", \"Surge\", \"Relief\", \"Resilience\", \"Evacuate\", \"Precautions\", \"Preparedness\", \"Cyclonic storm\", \"Typhoon warning\", \"Evacuation plan\", \"Weather alert\", \"Severe weather\", \"Cyclone aftermath\", \"Emergency response\", \"Tropical storm\", \"Cyclone's path\", \n",
    "\n",
    "]\n",
    "print([x.lower() for x in drought_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
